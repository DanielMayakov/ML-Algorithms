{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkNPdXJtuf9t"
   },
   "source": [
    "# Классификация с помощью kNN. Кластеризация K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWKvWqaKuf9v"
   },
   "source": [
    "В этом уроке речь пойдет о так называемых _метрических алгоритмах_, то есть об алгоритмах, построенных на вычислении расстояний между объектами. Для начала нам нужно определить, какими способами эти расстояния можно вычислять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zDVBc3sfuf9w"
   },
   "source": [
    "## Меры и метрики расстояния между объектами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tr0vgrRduf9x"
   },
   "source": [
    "Понятие метрических пространств, метрик и некоторые их примеры уже рассматривались вами на курсе по линейной алгебре в рамках обучения по профессии. В этом разделе мы вспомним, какие могут быть метрики и их применение в плоскости машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwD8b7Truf9y"
   },
   "source": [
    "Метрика является функцией, задающей расстояние в метрическом пространстве. Как мы помним из курса линейной алгебры, она должна удовлетворять трем аксиомам:\n",
    "\n",
    "1. $\\rho(x, y) \\geq 0, \\rho(x, y) = 0 \\Leftrightarrow x = y;$\n",
    "2. $\\rho(x, y) = \\rho (y, x);$\n",
    "3. $\\rho(x, y) \\leq \\rho(x, z) + \\rho(z, y).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLK_Bvxiuf9z"
   },
   "source": [
    "Евклидова метрика\n",
    "\n",
    "$$\\rho(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^{2}}$$\n",
    "\n",
    "и манхэттенская метрика\n",
    "\n",
    "$$\\rho(x, y) = \\sum_{i=1}^{n}|x_{i}-y_{i}|$$\n",
    "\n",
    "уже должны быть вам знакомы. \n",
    "\n",
    "Также можно показать обобщение этих двух метрик - метрика Минковского:\n",
    "\n",
    "$$\\rho(x, y) = \\left ( \\sum_{i=1}^{n}|x_{i}-y_{i}|^{q} \\right )^{\\frac{1}{q}}.$$\n",
    "\n",
    "При этом при $q = 1$ получаем манхэттенскую ($L_{1}$) метрику, при $q = 2$ - евклидову ($L_{2}$) метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VHDuUyFuf9z"
   },
   "source": [
    "Кроме этого, в метрических алгоритмах часто используются так называемые меры близости. В отличие от метрик, которые тем меньше, чем объекты более похожи, меры близости увеличиваются при увеличении похожести (близости) объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5Qvkxxauf90"
   },
   "source": [
    "Примером такой функции может быть _косинусное сходство (косинусная мера)_:\n",
    "\n",
    "$$\\text{cos}\\theta = \\frac{\\left \\langle x, y \\right \\rangle}{||x||\\cdot||y||} = \\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}y_{i}^{2}}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUl9cSZ9uf91"
   },
   "source": [
    "Из этой функции также вытекает _косинусное расстояние_:\n",
    "\n",
    "$$\\rho_{cos}(x, y) = 1 - \\text{cos}\\theta = 1 - \\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}\\sqrt{\\sum_{i=1}^{n}y_{i}^{2}}}.$$\n",
    "\n",
    "__Аналитический смысл__: объекты представлены в виде векторов, между двумя векторами образуется угол, значение косинуса этого угла – это и есть косинусная мера (следует из формулы скалярного произведения). \n",
    "\n",
    "__Почему эта функция - метрика?__\n",
    "\n",
    "* f(x, x) = 0 - потому что arccos(1)=0\n",
    "* f(x,y) = f(y,x) - по симметрии - очевидно, что угол между x и y равен углу между y и x\n",
    "* f(x,y) >= 0 - потому что мы рассматриваем углы от 0 до 180 градусов\n",
    "* неравенство треугольника (физический смысл)\n",
    "\n",
    "Именно такая формула используется в функции `scipy.spatial.distance.cosine()` из библиотеки scipy, возвращающая косинусное расстояние между двумя векторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVefMMeWuf91"
   },
   "source": [
    "Косинусная мера часто используется в анализе текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3S2TJ8luf92"
   },
   "source": [
    "Кроме косинусной меры в прикладных задачах могут использоваться и другие функции - коэффициент Дайса, коэффициент Жаккара, коэффициент перекрытия (см. доп. материалы)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzwjKFWpuf93"
   },
   "source": [
    "Формула нахождения косинусной меры похожа на _коэффициент корреляции_, который также может быть использован как мера близости и используется обычно в рекомендательных системах:\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{n}((x_{i} - \\bar{x})(y_{i} - \\bar{y}))}{\\sqrt{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}}\\sqrt{\\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29Ii-h2Auf94"
   },
   "source": [
    "## Алгоритм kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gROL4l_uf94"
   },
   "source": [
    "Начнем обсуждение метрических алгоритмов с алгоритма kNN. Данный алгоритм является еще одним способом решения задачи классификации. Расшифровывается его название как \"k ближайших соседей (k nearest neighbours)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqQdBzZyuf95"
   },
   "source": [
    "Суть его довольно проста и заключается в принципе отнесения объекту к тому классу, представители которого преобладают рядом с ним. Таким образом, упрощенно алгоритм классификации выглядит следующим образом:\n",
    "\n",
    "- найти расстояние от объекта $u$ до каждого из объектов $x$ обучающей выборки;\n",
    "- выбрать $k$ объектов, расстояние до которых минимально;\n",
    "- отнести объект к классу, к которому относится большинство из выбранных $k$ ближайших соседей:\n",
    "\n",
    "$$a(u) = \\underset{y}{\\text{argmax}}\\sum_{i=1}^{k}[y_{u}^{(i)}=y],$$\n",
    "\n",
    "то есть провести голосование.\n",
    "\n",
    "При этом метод можно адаптировать под регрессию: в этом случае находится не метка класса, а среднее значение ответа среди $k$ соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSwxja77uf96"
   },
   "source": [
    "Интересной особенностью метода является то, что __на этапе обучения не строится модель__, а просто запоминается обучающая выборка. Вычисления начинаются именно на этапе решения задачи классификации конкретного объекта (поэтому этот алгоритм можно назвать ленивым)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_6VaBrnuf96"
   },
   "source": [
    "Логичным усовершенствованием алгоритма kNN является добавление соседям весов (так называемое \"взвешенное голосование\"), зависящих от их порядкового номера или расстояния до классифицируемого объекта (чем ближе объект обучающей выборки, тем больше его вес).\n",
    "\n",
    "От номера соседа $i$ веса можно определять как:\n",
    "\n",
    "- $w(i) = q^{i}$,   $q \\in (0,1)$;\n",
    "\n",
    "\n",
    "- $w(i) = \\frac{1}{i}$;\n",
    "\n",
    "\n",
    "- $w(i) = \\frac{1}{(i+a)^{b}}$;\n",
    "\n",
    "\n",
    "- $w(i) = \\frac{k + 1 - i}{k}$.\n",
    "\n",
    "\n",
    "От расстояния $d$ веса можно определять как:\n",
    "\n",
    "- $w(d) = q^{d}$,   $q \\in (0,1)$;\n",
    "\n",
    "\n",
    "- $w(d) = \\frac{1}{(d+a)^{b}}$, брать вес $\\frac{1}{d}$ по аналогии с номером соседа - неудачное решение, так как при $d = 0$ вес будет бесконечно большим, что приводит к переобучению\n",
    "\n",
    "\n",
    "- $w(d) = \\begin{cases}\n",
    "\\frac{d(z_{k}, x) - d(z_{i}, x)}{d(z_{k}, x) - d(z_{1}, x)}, & d(z_{k}, x) \\neq d(z_{1}, x) \\\\ \n",
    "1, & d(z_{k}, x) = d(z_{1}, x)\n",
    "\\end{cases}$.\n",
    "\n",
    "Существуют и другие способы вычисления весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYubreR8uf97"
   },
   "source": [
    "Часто перед работой по алгоритму kNN требуется проводить нормализацию признаков, так как они могут иметь разные единицы измерения, что может искажать расстояние между объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xYJJr-Wuf98"
   },
   "source": [
    "В общем и целом получается, что при работе с алгоритмом kNN исследователю требуется подобрать три параметра - количество соседей k, метрика расстояния и способ вычисления весов. Для получения лучшего качества работы алгоритма эти параметры нужно подбирать на отложенной выборке или при помощи кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANWmZgT2uf99"
   },
   "source": [
    "Реализуем алгоритм kNN с помощью Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChmkZ71kuf9-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5FaldH6uf-B"
   },
   "source": [
    "Загрузим один из \"игрушечных\" датасетов из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_49Y22E9uf-C"
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Для наглядности возьмем только первые два признака (всего в датасете их 4)\n",
    "X = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpwmydFPuf-F"
   },
   "source": [
    "Разделим выборку на обучающую и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACYClLghuf-F"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Johgj2p6uf-J",
    "outputId": "081c6165-bd71-4800-9ab1-78646f5489fd"
   },
   "outputs": [],
   "source": [
    "cmap = ListedColormap(['red', 'green', 'blue'])\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59xP1kfUuf-O"
   },
   "source": [
    "Используем евклидову метрику. Реализуем функцию для ее подсчета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKipIKeGuf-P"
   },
   "outputs": [],
   "source": [
    "def e_metrics(x1, x2):\n",
    "    \n",
    "    distance = 0\n",
    "    for i in range(len(x1)):\n",
    "        distance += np.square(x1[i] - x2[i])\n",
    "    \n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tvFJ8-ouf-T"
   },
   "source": [
    "Реализуем алгоритм поиска k ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIKuDr9huf-V"
   },
   "outputs": [],
   "source": [
    "def knn(x_train, y_train, x_test, k):\n",
    "    \n",
    "    answers = []\n",
    "    for x in x_test:\n",
    "        test_distances = []\n",
    "            \n",
    "        for i in range(len(x_train)):\n",
    "            \n",
    "            # расчет расстояния от классифицируемого объекта до\n",
    "            # объекта обучающей выборки\n",
    "            distance = e_metrics(x, x_train[i])\n",
    "            \n",
    "            # Записываем в список значение расстояния и ответа на объекте обучающей выборки\n",
    "            test_distances.append((distance, y_train[i]))\n",
    "        \n",
    "        # создаем словарь со всеми возможными классами\n",
    "        classes = {class_item: 0 for class_item in set(y_train)}\n",
    "        \n",
    "        # Сортируем список и среди первых k элементов подсчитаем частоту появления разных классов\n",
    "        for d in sorted(test_distances)[0:k]:\n",
    "            classes[d[1]] += 1\n",
    "            \n",
    "        # Записываем в список ответов наиболее часто встречающийся класс\n",
    "        answers.append(sorted(classes, key=classes.get)[-1])\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DVrbUPyuf-X"
   },
   "source": [
    "Напишем функцию для вычисления точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eK2Bn_7ouf-Y"
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    return (sum(pred == y) / len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SprqYVBQuf-a"
   },
   "source": [
    "Проверим работу алгоритма при различных k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8K98g57quf-a",
    "outputId": "be768ef7-9825-4ffb-9dc6-516565e92263"
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "\n",
    "y_pred = knn(X_train, y_train, X_test, k)\n",
    "\n",
    "print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YahR_F7xuf-d"
   },
   "source": [
    "Построим график распределения классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vN4DDyMauf-e"
   },
   "outputs": [],
   "source": [
    "def get_graph(X_train, y_train, k):\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])\n",
    "\n",
    "    h = .02\n",
    "\n",
    "    # Расчет пределов графика\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Получим предсказания для всех точек\n",
    "    Z = knn(X_train, y_train, np.c_[xx.ravel(), yy.ravel()], k)\n",
    "\n",
    "    # Построим график\n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Добавим на график обучающую выборку\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(f\"Трехклассовая kNN классификация при k = {k}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0twiiFZuf-i",
    "outputId": "81463ebb-c1a8-47ff-ff83-a6b1dd4aec39"
   },
   "outputs": [],
   "source": [
    "get_graph(X_train, y_train, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HyO60dnuf-l",
    "outputId": "41fd701f-8531-4efb-90eb-9f394051e35a"
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "y_pred = knn(X_train, y_train, X_test, k)\n",
    "\n",
    "print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuxShjfxuf-n",
    "outputId": "21f96cdd-2d09-4cf2-d943-7835c2e222e7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_graph(X_train, y_train, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v276kSfFuf-q",
    "outputId": "0dc08804-d25b-4764-c798-0287ab74690e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 5\n",
    "\n",
    "y_pred = knn(X_train, y_train, X_test, k)\n",
    "\n",
    "print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0uMulUpuf-u",
    "outputId": "16611f87-8ab1-4ac2-f4ef-79dedd7847db"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_graph(X_train, y_train, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-FpThNiuf-0",
    "outputId": "332854a5-a1e9-4e4e-dc6e-b5b15f1acd81"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "y_pred = knn(X_train, y_train, X_test, k)\n",
    "\n",
    "print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1nSITJluf-3",
    "outputId": "49d8fca3-4aaf-4c74-9ef8-15133b13f7d8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "get_graph(X_train, y_train, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHVr1RTKuf-6"
   },
   "source": [
    "При увеличении k мы на графиках наблюдаем, как алгоритм меньше концентрируется на выбросах, однако, точность на тестовой выборке при этом увеличивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbAXUnqUuf-7"
   },
   "source": [
    "Рассматриваемый метод, несмотря на положительные стороны в виде легкости интерпретации, простоты и удобства использования, обладает некоторыми минусами, в частности, он плохо работает на датасетах с большим количеством признаков.\n",
    "\n",
    "Например, если мы имеем три объекта, при этом второй отличается от первого только значением одного признака, но значительно, а третий отличается от первого незначительно в каждом признаке, расстояния от первого объекта до второго и третьего могут совпадать. Несущественные различия в каждом признаке могут иметь большее значение, чем большое различие в одном признаке. Такое поведение в ряде случаев будет нежелательным. \n",
    "\n",
    "Второй пример - случай, когда количество признаком сравнимо с количеством объектов. В этом случае может возникнуть ситуация, когда расстояния между любыми двумя объектами почти одинаковы. В двумерном пространстве (на плоскости) три точки могут располагаться по вершинам равностороннего треугольника, при этом расстояния между ними будут равны; в трехмерном пространстве то же самое справедливо для четырех точек на вершинах тетраэдра - расстояние между любыми двумя точками будет одинаково. В общем случае это означает, что в $n$-мерном пространстве можно выбрать $n+1$ точку так, чтобы расстояние между любыми двумя точками было одинаковым.\n",
    "\n",
    "Третий пример - так называемое \"проклятие размерности\". Суть его заключается в том, что при наличии $n$ бинарных признаков в пространстве признаков будет возможно $2^{n}$ различных объектов вида $x = (0,1,0,0,1,...,1)$, и размер обучающей выборки, необходимый, чтобы описать все пространство объектов (то есть все возможные комбинации таких признаков) также будет порядка $2^{n}$. Чтобы покрыть не все пространство, а долю объектов $\\alpha$ (то есть долю объема $\\alpha$) нужно будет описать гиперкуб с длиной ребра $e_{p} = \\alpha^{n}$. Например, в 10-мерном пространстве признаков чтобы покрыть 1% объема нужно взять гиперкуб с длиной $e_{10}(0,01) = 0.63$, то есть взять окрестность длиной больше половины ребра исходного пространства. Чем больше признаков, тем меньше будет область, которая покрывается во время поиска на заданном расстоянии. Таким образом, при сохранении требований по точности нахождения объекта в пространстве, количество требуемых данных для этого при увеличении количества признаков растет экспоненциально (подробнее про это явление см. в доп. материалах)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6H4U6Fcuf-8"
   },
   "source": [
    "## Обучение без учителя. Кластеризация. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X38-NciTuf-9"
   },
   "source": [
    "До этого мы рассматривали методы обучения с учителем, то есть задачи, в которых изначально есть размеченная обучающая выборка данных с известными ответами на них и тестовая выборка, на которой проверяется качество алгоритма. В этом разделе речь пойдет об _обучении без учителя (unsupervised learning)_ - случае, когда в роли обучающей выборки выступает просто набор объектов $x_{1},...,x_{l}$, и он же выступает в роли тестовой выборки, а задача состоит в проставлении меток $y_{1},...,y_{l}$ так, что бы объекты с одной и той же меткой были похожи, а с разными - нет. То есть все объекты в пространстве признаков нужно разделить на группы, найти структуру в данных. Это и называется _кластеризацией_. Если раньше мы имели примеры ответов $y$, то сейчас их нет, и нужно строить свои отображения $x \\rightarrow y$, отвечающие некоторым свойствам, например, тому, что похожие объекты отображаются в одну метку, а непохожие - в разные. По-простому задачу кластеризации можно сформулировать так: имеется множество точек, которые скапливаются в сгустки, нужно найти возможность относить точки к тому или иному сгустку и предсказывать, в какой сгусток попадет новая точка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cba7q4vuf--"
   },
   "source": [
    "Примерами кластеризации может быть группирование новостей по темам, музыки по жанрам, клиентов по типу поведения и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5fLWrfFuf-_"
   },
   "source": [
    "Логично возникает вопрос, как измерять качество кластеризации. Есть большое количество инструментов оценки качества кластеризации, они разделяются на _внутренние_ (основанные только на свойствах выборки и кластеров) и _внешние_ (использующие данные об истинном распределении объектов по кластерам, если оно известно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1Oyn7heuf_A"
   },
   "source": [
    "Примерами внутренних метрик могут быть:\n",
    "\n",
    "- Внутрикластерное расстояние (также называется компактностью кластеров, cluster cohesion): $$\\sum_{k=1}^{K}\\sum_{i=1}^{l}[a(x_{i})=k]\\rho(x_{i}, c_{k}),$$ где $K$ - количество кластеров, $c_{k}$ - центр кластера. Этот функционал нужно минимизировать, так как в идеальном случае все объекты в одном кластере одинаковы, и расстояние между ними равно нулю.\n",
    "\n",
    "\n",
    "- Межкластерное расстояние (отделимость кластеров, cluster separation): $$\\sum_{i,j=1}^{l}[a(x_{i}) \\neq a(x_{j})]\\rho(x_{i}, x_{j}).$$ Этот функционал наоборот нужно максимизировать, так как объекты из разных кластеров должны максимально различаться, то есть иметь максимальное расстояние между собой.\n",
    "\n",
    "    - Часто используются те же формулы, но включающие не расстояние ρ, а его квадрат, получая квадратичное внутрикластерное и межкластерное расстояние:\n",
    "\n",
    "    $$\\sum_{k=1}^{K}\\sum_{i=1}^{l}[a(x_{i})=k]\\rho^{2}(x_{i}, c_{k}),$$\n",
    "    $$\\sum_{i,j=1}^{l}[a(x_{i}) \\neq a(x_{j})]\\rho^{2}(x_{i}, x_{j}).$$\n",
    "\n",
    "\n",
    "- Среднее внутрикластерное расстояние (среднее расстояние внутри каждого кластера, просуммированное по всем кластерам) и среднее межкластерное расстояние (минимизируется и максимизируется, соответственно, по аналогии с двумя первыми функционалами):\n",
    "\n",
    "    $$\\sum_{k=1}^{K}\\frac{1}{|k|}\\sum_{i=1}^{l}[a(x_{i})=k]\\rho(x_{i}, c_{k}),$$\n",
    "    $$\\frac{1}{K}\\sum_{i,j=1}^{l}[a(x_{i}) \\neq a(x_{j})]\\rho(x_{i}, x_{j}),$$ где $|k|$ - количество элементов в кластере под номером $k$.\n",
    "\n",
    "    - По аналогии с квадратичным внутрикластерным и межкластерным расстоянием - среднее квадратичное внутрикластерное и межкластерное расстояние.\n",
    "\n",
    "    $$\\sum_{k=1}^{K}\\frac{1}{|k|}\\sum_{i=1}^{l}[a(x_{i})=k]\\rho^{2}(x_{i}, c_{k}),$$\n",
    "    $$\\frac{1}{K}\\sum_{i,j=1}^{l}[a(x_{i}) \\neq a(x_{j})]\\rho^{2}(x_{i}, x_{j}).$$ \n",
    "\n",
    "- Отношение внутрикластерного и межкластерного расстояний (или средних), минимизируется.\n",
    "\n",
    "\n",
    "- Индекс Данна (Dunn Index): $$\\frac{\\text{min}_{1\\leq k \\leq k' \\leq K}d(c_{k},c_{k'})}{\\text{max}_{1\\leq k \\leq K} d(k)},$$ где $d(c_{k},c_{k'})$ - расстояние между кластерами $k$ и $k'$ (между их центрами), $d(k)$ - внутрикластерное расстояние для кластера $k$. Этот функционал требуется максимизировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86Fs9Fbeuf_A"
   },
   "source": [
    "Внешние метрики используются, если есть дополнительные знания о кластеризуемой выборке, например, известно истинное распределение по кластерам. Задачу можно рассматривать как задачу многоклассовой классификации с использованием соответствующих метрик. В этом случае примерами могут быть:\n",
    "\n",
    "- Rand Index: $$Rand = \\frac{TP+FN}{TP+TN+FP+FN}, $$ входящие в формулу обозначения мы встречали при изучении классификации и матриц ошибок. Здесь это количество пар объектов $(x_{i}.x_{j})$, которые принадлежат одному кластеру и одному классу (TP), одному кластеру, но разным классам (TN), разным кластерам, но одному классу (FP), разным кластерам и разным классам (FN). Этот индекс оценивает, сколько пар объектов, находившихся в одном классе, и пар объектов, находившихся в разных классах, сохранили это состояние после работы алгоритма. Он принимает значение от 0 до 1, где 1 - полное совпадение полученных кластеров и исходными классами, 0 - полное отсутствие совпадений.\n",
    "\n",
    "\n",
    "- Jaccard Index: $$Jaccard = \\frac{TP}{TP+TN+FP}.$$ Этот индекс похож на предыдущий, но он не учитывает пары объектов, находящихся в разных кластерах и разных классах. Имеет такую же область определения, как и Rand Index.\n",
    "\n",
    "\n",
    "- F-мера: $$\\sum_{j}\\frac{l_{j}}{l}\\underset{i}{\\text{max}}\\left(\\frac{2\\cdot precision(i,j) \\cdot recall(i,j)}{precision(i,j) + recall(i,j)}\\right),$$ здесь используются также знакомые из темы классификации параметры точности (precision) и полноты (recall). В данном случае они определяются как: $$precision(i,j)=\\frac{l_{ij}}{l_{i}},$$ $$recall(i,j)=\\frac{l_{ij}}{l_{j}},$$ где $l_{ij}$ - количество объектов $x_{n}$, принадлежащих кластеру $k_{i}$ и классу $c_{j}$; $l_{i} = |k_{i}|$ - размер кластера $k_{i}$; $l_{j} = |c_{j}|$ - размер класса $c_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5POKW_CGuf_B"
   },
   "source": [
    "Про другие внутренние и внешние метрики качества кластеризации можно почитать в дополнительных материалах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oq8LyMncuf_B"
   },
   "source": [
    "### Алгоритм K-means (K-средних)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Slqzj4Siuf_C"
   },
   "source": [
    "Одним из самых простых и популярных алгоритмов кластеризации является алгоритм _K-means (K-средних)_. Заключается он в следующих шагах:\n",
    "\n",
    "1 . Выбрать количество кластеров $k$, на которые будут делиться данные.\n",
    "\n",
    "2 . Случайным образом выбрать в пространстве данных $k$ точек $c_{k}$ (центроидов) - центров будущих кластеров.\n",
    "\n",
    "3 . Для каждой точки из выборки посчитать, к какому из центроидов она ближе.\n",
    "\n",
    "4 . Переместить каждый центроид в центр выборки, отнесенной к этому центроиду, определив его как среднее арифметическое всех точек кластера:\n",
    "\n",
    "$$c_{k} = \\frac{\\sum_{i=1}^{l}[a(x_{i})=k]x_{i}}{\\sum_{i=1}^{l}[a(x_{i}) = k]}.$$\n",
    "5 . Повторить шаги 3-4 до сходимости алгоритма (обычно это оценивается по величине смещения центроида после каждого шага - сходимость означает непревышение смещения какого-то заданного значения).\n",
    "\n",
    "Результат работы алгоритма значительно зависит от начального выбора центроидов. Существует много методик их выбора, наиболее удачным из которых считается k-means++. Он заключается в последовательном выборе начальных приближений так, что вероятность выбрать в качестве центроида следующую точку пропорциональна квадрату расстояния от нее до ближайшего центроида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2v7sbNMFuf_D"
   },
   "source": [
    "Проблемой метода также является необходимость знать число кластеров, на которые будет делиться выборка. В случае, когда это число неизвестно, вариантом ее решения может быть последовательная кластеризация на разное число кластеров (например, от 1 до 10) с последующим анализом качества работы алгоритма, например, по сумме квадратов внутрикластерных расстояний $$\\sum_{k=1}^{K}\\sum_{i \\in C_{k}}\\rho(x_{i}, c_{k})^{2}$$ - выбирается такое число кластеров, начиная с которого при увеличении количества кластеров функционал падает незначительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEhfY3kMuf_E"
   },
   "source": [
    "Сделаем простую реализацию алгоритма K-means своими руками на Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDbkTkTYuf_F"
   },
   "source": [
    "Вначале сгенерируем три облака точек с помощью инструментов `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRhCTQdZuf_G"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import random\n",
    "\n",
    "X, y = make_blobs(n_samples=100, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv0J1xcDuf_H"
   },
   "source": [
    "Изобразим точки на графике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6VqFOARuf_I",
    "outputId": "0b4f8980-a699-4bc3-9256-6c6f6d029839"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a421abi0uf_L"
   },
   "source": [
    "В качестве метрики расстояния будем использовать евклидово расстояние, функция для которого была написана ранее. В качестве центроидов выберем первые k элементов датасета. Реализуем основной цикл алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LSTomiyuf_L"
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k, max_iterations, min_distance):    \n",
    "    # инициализируем центроиды как первые k элементов датасета\n",
    "    centroids = [data[i] for i in range(k)]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # Создадим словарь для классификации\n",
    "        classes = {i: [] for i in range(k)}\n",
    "        \n",
    "        # классифицируем объекты по центроидам\n",
    "        for x in data:\n",
    "            # определим расстояния от объекта до каждого центроида\n",
    "            distances = [e_metrics(x, centroid) for centroid in centroids]\n",
    "            # отнесем объект к кластеру, до центроида которого наименьшее расстояние\n",
    "            classification = distances.index(min(distances))\n",
    "            classes[classification].append(x)\n",
    "        \n",
    "        # сохраним предыдущие центроиды в отдельный список для последующего сравнения сновыми\n",
    "        old_centroids = centroids.copy()\n",
    "        \n",
    "        # пересчитаем центроиды как среднее по кластерам\n",
    "        for classification in classes:\n",
    "            centroids[classification] = np.average(classes[classification], axis=0)\n",
    "            \n",
    "        # сравним величину смещения центроидов с минимальной\n",
    "        optimal = True\n",
    "        for centroid in range(len(centroids)):\n",
    "            if np.sum(abs((centroids[centroid] - old_centroids[centroid]) / old_centroids * 100)) > min_distance:\n",
    "                optimal = False\n",
    "                \n",
    "        # если все смещения меньше минимального, останавливаем алгоритм  \n",
    "        if optimal:\n",
    "            break\n",
    "    \n",
    "    return old_centroids, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hF5nlCxFuf_O"
   },
   "source": [
    "Напишем функцию для визуализации кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5RysDcPuf_O"
   },
   "outputs": [],
   "source": [
    "def visualize(centroids, classes):\n",
    "    colors = ['r', 'g', 'b']\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    # нанесем на график центроиды\n",
    "    for centroid in centroids:\n",
    "        plt.scatter(centroid[0], centroid[1], marker='x', s=130, c='black')\n",
    "        \n",
    "    # нанесем объекты раскрашенные по классам\n",
    "    for class_item in classes:\n",
    "        for x in classes[class_item]:\n",
    "            plt.scatter(x[0], x[1], color=colors[class_item])\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hsy8q0FOuf_Q"
   },
   "outputs": [],
   "source": [
    "# определим максимальное количество итераций\n",
    "max_iterations = 1\n",
    "\n",
    "# и минимальное расстояние между центроидами до пересчета и после него, при котором нужно остановить алгоритм\n",
    "min_distance = 1e-4\n",
    "\n",
    "# сразу определим известное нам количество кластеров\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykFQU-1guf_S"
   },
   "source": [
    "Проверим результат алгоритма после одной итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzJTqoBuuf_T",
    "outputId": "7b5eb959-2373-4657-f33d-e6f40a2ecd4c"
   },
   "outputs": [],
   "source": [
    "centroids, clusters = kmeans(X, k, max_iterations, min_distance)\n",
    "\n",
    "visualize(centroids, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_Zi1CuPuf_W"
   },
   "source": [
    "Проверим работу алгоритма при различном числе итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt7c3Vdxuf_X",
    "outputId": "f95a9a45-7366-4837-fece-6e25d9163c8b"
   },
   "outputs": [],
   "source": [
    "max_iterations = 3\n",
    "\n",
    "centroids, clusters = kmeans(X, k, max_iterations, min_distance)\n",
    "\n",
    "visualize(centroids, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfMBMBlQuf_a",
    "outputId": "959c1659-47cd-4df4-af49-74d2f591a3b7"
   },
   "outputs": [],
   "source": [
    "max_iterations = 5\n",
    "\n",
    "centroids, clusters = kmeans(X, k, max_iterations, min_distance)\n",
    "\n",
    "visualize(centroids, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfdoJFyUuf_c",
    "outputId": "536aeb9d-4b54-4333-867e-1473a026deae"
   },
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "\n",
    "centroids, clusters = kmeans(X, k, max_iterations, min_distance)\n",
    "\n",
    "visualize(centroids, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_E1dFmeuf_e"
   },
   "source": [
    "Видно, как при увеличении количества итераций центроиды перемещаются в центр образующихся кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fb9vr5iXuf_e"
   },
   "source": [
    "Как и говорилось ранее, метод K-means очень чувствителен к выбору начальных центров кластеров, и это является одним из его основных минусов, среди которых также сложность работы с разными формами кластеров (они, например, могут быть вытянутыми, образовывать фигуры и т.п.). В связи с этим существуют другие методы кластеризации, о которых можно прочитать в дополнительных материалах. Там же есть сравнение работы алгоритмов с кластерами различных форм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_LkljD0uf_f"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zw4kpwm0uf_f"
   },
   "source": [
    "1. Факторный, дискриминантный и кластерный анализ: Пер. с англ./Дж.-О. Ким, Ч. У. Мьюллер, У. Р. Клекка и др.; Под ред. И. С. Енюкова. — М.: Финансы и статистика, 1989.— 215 с.\n",
    "2. [Об использовании мер сходства при анализе документации](http://ceur-ws.org/Vol-803/paper18.pdf)\n",
    "3. [KNN Algorithm](https://medium.com/datadriveninvestor/knn-algorithm-and-implementation-from-scratch-b9f9b739c28f)\n",
    "4. [Проклятие размерности](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D1%80%D0%BE%D0%BA%D0%BB%D1%8F%D1%82%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8); ([Иллюстрация](https://www.youtube.com/watch?v=R2hQIJb0Lis))\n",
    "5. [Оценка качества кластеризации](http://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B0_%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B0_%D0%B2_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B5_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8)\n",
    "6. [Обзор алгоритмов кластеризации данных](https://habr.com/ru/post/101338/)\n",
    "7. [Работа алгоритмов кластеризации на классах разной формы](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__kNN__\n",
    "* Метрический алгоритм классификации по \"ближайшим соседям\"\n",
    "* Разновидности: ближайший сосед, k ближайших соседей, k взвешанных ближайших соседей\n",
    "* Алгоритм интерпретируем\n",
    "* В основе алгоримта лежит _гипотеза компактности_: если мера сходства объектов введена достаточно удачно, то схожие объекты гораздо чаще лежат в одном классе, чем в разных\n",
    "* Проблемы и их решения\n",
    "    * Выбор метрики __->__ нет единого алгоритма, позволяющего выбрать метрику, поэтому используют что-то из \"стандартного\" (например, Евклидова)\n",
    "    * Выбор числа соседей k __->__ подбор на основании кросс-валидации, чаще даже по контролю одного объекта (leave-one-out cross-validation)\n",
    "    * Выбросы/шум __->__ ручной отсев, придание меньшего веса, либо поиск типичных представителей классов (эталонов) алгоритмом STOLP, тогда объект можно классифицировать по ближайшему эталону\n",
    "    * Большие выборки __->__ уменьшение выборки за счет удаления неинформативных объектов (например, оставить только эталоны), либо более быстрые структуры данных (например, kd-деревья)\n",
    "    * Много признаков - \"проклятие размерности\" - суммы большого числа отклонений по отдельным признакам с большой вероятностью имеют очень близкие значения (согласно закону больших чисел), то есть в пространстве высокой размерности все объекты примерно одинаково далеки друг от друга __->__ предварительный отбор признаков (feature selection)\n",
    "\n",
    "__k-means__\n",
    "* Кластеризует неразмеченные объекты, помогая найти структуру в данных\n",
    "* Использует метрику близости, является метрическим алгоритмом\n",
    "* Можно использовать для генерации нового признака - номера кластера\n",
    "* Проблемы и их решения\n",
    "    * Алгоритм чувствителен к выбору начальных центров кластеров __->__ как вариант, на начальном этапе принимать в качестве центров самые отдаленные точки\n",
    "    * Необходимо заранее знать количество кластеров __->__ выбор исходя из природы данных, либо оценка исходя из распределений признаков или 2D представления объектов (PCA)\n",
    "    * Не справляется с задачей, когда объект принадлежит к разным кластерам в равной степени или не принадлежит ни одному __->__ выбирать случайно ответ, либо ввести более сложную функцию весов\n",
    "    * Не гарантируется достижение глобального минимума суммарного квадратичного отклонения, а только одного из локальных минимумов (пример на картинке ниже) __->__ инициализировать начальные центры не случайно\n",
    "    \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/1920px-K-means_convergence_to_a_local_minimum.png\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Д/з"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. К алгоритму kNN, реализованному на уроке, реализовать добавление весов соседей по любому из показанных на уроке принципов.\n",
    "2. Написать функцию подсчета метрики качества кластеризации как среднее квадратичное внутрикластерное расстояние и построить график ее зависимости от k (взять от 1 до 10) для выборки данных из данного урока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Можно ли сбалансировать классы в задаче классификации метрическими алгоритмами?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, даже нужно :) Метрические алгоритмы позволяют делать как __up-sampling__ (создавая новые синтетические объекты), так и __down-sampling__ (например, заменяя, кластеры объектов их центроидами). См. библиотеку [imblearn](https://imbalanced-learn.readthedocs.io/en/stable/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Как можно решить проблему с сильной зависимостью результатов k-means от начальных значений центров кластеров?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть усовершенствованная версия алгоритма __k-means++__, в котором центры инициализируются оптимальным образом:\n",
    "<img src=\"images/L7_Q2_KM++.png\" style=\"width: 1000px;\">\n",
    "\n",
    "Также есть алгоритм __C-means__ (a.k.a. Fuzzy clustering, Soft k-means)\n",
    "<img src=\"images/L7_Q2_CM.png\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Какова сложность алгоритмов k-means и c-means?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L7_Q3.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Нужно ли масштабировать/нормировать данные для метрических алгоритмов?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, иначе признаки с наибольшими значениями будут доминировать в метрике, а остальные признаки, фактически, учитываться не будут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Какие применения в реальной жизни могут быть у kNN и k-means?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__kNN__\n",
    "\n",
    "1. Рекомендательные системы - схожим пользователям рекомендуем одинаковые товары/услуги.\n",
    "2. Обнаружение фрода (мошенничества) - новые случаи мошенничества похожи на те, которые происходили ранее\n",
    "3. Предсказание отклика клиентов для новых клиентов по историческим данным.\n",
    "4. Медицина - классификация пациентов по набору показателей и определение потенциальной \"склонности\" к схожему диагнозу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__k-means__\n",
    "\n",
    "1. В задачах глубокого обучения и машинного зрения - создания так называемых фильтров (ядер свёртки, словарей)\n",
    "2. Кластеризация пользователей по степени активности или платежеспособности для персонализированных подходов\n",
    "3. Распознавания воздушных объектов - кластеризация объектов в схожие группы (истребитель, бомбардировщик, пассажирский)\n",
    "4. Векторное квантование цвета - уменьшения цветовой палитры изображения с фиксированным количеством цветов k."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_7(edited).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
