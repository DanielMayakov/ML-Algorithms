{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLiBxjBJ9IMs"
   },
   "source": [
    "# Урок 1. Алгоритм линейной регрессии. Градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgfZohhE9IMz"
   },
   "source": [
    "## 0. Основные понятия и обозначения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCRpRG3-9IM0"
   },
   "source": [
    "В предыдущих курсах студенты уже знакомились с признаковыми описаниями, основными алгоритмами машинного обучения и Python-библиотеками, используемыми для решения задач в этой области. Однако, перед началом изучения программы курса давайте повторим основные понятия и обозначения, используемые в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pzSl1NM9IM2"
   },
   "source": [
    "Машинное обучение - дисциплина, заключаящаяся в извлечении знаний из известных данных. Машинное обучение - это раздел математики, поэтому в нем мы будем, помимо всего прочего, работать с формулами.\n",
    "\n",
    "_Объект_ - то, для чего нужно сделать предсказание. Например, в задаче распознавания спам-почты объектом будет являться письмо. Объекты обозначаются буквой $x$. Множество всех объектов, для которых может потребоваться сделать предсказания, называется _пространством объектов_ и обозначается $\\mathbb{X}$.\n",
    "\n",
    "_Ответ_ - то, что нужно предсказать. В том же примере распознавания спама ответом будет является информация о том, является письмо спамом или нет. Ответы обозначаются буквой $y$ (можно сказать, что $y = y(x)$, так как ответ зависит от объекта). _Пространство ответов_ - множество всех ответов, с которыми мы можем работать. Оно обозначается $\\mathbb{Y}$. В примере задачи распознавания спама оно состоит из двух элементов: $+1$ и $-1$ (означающие, что письмо является и не является спамом, соответственно).\n",
    "\n",
    "Для реализации машинного обучения компьютеру нужно \"объяснить\" объекты, которые в первоначальном виде он понять не может, с помощью сущностей, ему понятных, например, чисел. Для этого вводится понятие _признаков_. Признак - это некая числовая характеристика объекта. Совокупность всех признаков объекта $x = (x^{1}, x^{2},..., x^{d})$ называется его _признаковым описанием_. Оно является $d$-мерным вектором, то есть к нему можно применять все операции, описанные линейной алгеброй."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eb5uTrO89IM4"
   },
   "source": [
    "Множество значений $i$-го признака будем обозначать $D_{i}$. Существует множество различных видов признаков:\n",
    "\n",
    "- _Бинарные признаки_ принимают два значения: $D_{i} = \\{0,1\\}$. Примером  в задаче кредитного скоринга может служить ответ, выше ли доход клиента определенной установленной суммы. При положительном ответе признак полагается равным 1, при отрицательном - 0.\n",
    "\n",
    "- _Вещественные признаки_ могут принимать в качестве значений все вещественные числа: $D_{i} = \\mathbb{R}$. Примерами могут выступать возраст человека, заработная плата, количество звонков в колл-центр в месяц и т.д.\n",
    "\n",
    "- _Категориальные признаки_ - это такие признаки, значения которых можно сравнивать только на равенство, и нельзя на \"больше-меньше\". В этом случае $D_{i}$ - неупорядоченное множетсво. Примерами таких признаков могут выступать город, в котором родился клиент банка, или его образование.\n",
    "\n",
    "- _Порядковые признаки_ - частный случай категоиральных признаков. В этом случае $D_{i}$ - упорядоченное множество. Признаки можно сравнивать между собой, но нельзя определить расстояние между ними. Например, то же образование, но с введенным осмысленным порядком (высшее образование больше среднего профессионального, которое в свою очередь больше среднего и т.д.)\n",
    "\n",
    "- _Множествозначные признаки_ - признаки, значения которых на объекте являются подмножеством некоторого множества. Например, в задачах анализа текстов таким признаком является множество слов, которые входят в текст. Оно является подмножеством большого словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pQODZa09IM6"
   },
   "source": [
    "В первой части нашего курса мы рассмотрим алгоритмы _обучения с учителем_ или _контролироемого обучения (supervised learning)_. Данный метод заключается в восстановлении общей закономерности по конечному числу известных примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iABsxWwB9IM6"
   },
   "source": [
    "Центральным понятием машинного обучения является _обучающая выборка_. Это примеры, на основе которых мы планируем строить общую закономерность. Она обозначается $X$ и состоит из $l$ пар объектов $x_{i}$ и известных ответов $y_{i}$:\n",
    "\n",
    "$$X = (x^{i}, y_{i})^l_{i=1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0Fh0yJn9IM7"
   },
   "source": [
    "Функция, отображающая пространство объектов $\\mathbb{X}$ в пространство ответов $\\mathbb{Y}$, помогающая нам делать предсказания, называется _алгоритмом_ или _моделью_ и обозначается $a(x)$. Она принимает на вход объект и выдает ответ.\n",
    "\n",
    "В нашем примере распознавания спама такой моделью может являться линейный алгоритм (сумма всех признаков с некоторыми коэффициентами, также называемыми весами, $w_{i}$ c прибавлением константного коэффициента $w_{0}$):\n",
    "\n",
    "$$a(x) = sign(w_{0} + w_{1}x^{1}+w_{2}x^{2}+...+w_{d}x^{d}).$$\n",
    "\n",
    "Признаками в ней могут выступать наличие определенных слов в письме, наличие отправителя в адресной книге получателя, дата и время отправления, наличие фишинговых url и т.д. Операция взятия знака от этого выражения берется так как в примере распознавания спама пространство ответов состоит всего из двух элементов, как говорилось ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVlLfs339IM8"
   },
   "source": [
    "Для решения определенной задачи не все алгоритмы одинаково хорошо подходят. Для определения наиболее подходящего алгоритма введена характеристика, называемая _функционалом ошибки_ $Q(a, X)$. Он принимает на вход алгоритм и выборку и сообщает, насколько хорошо данный алгоритм работает на данной выборке. В примере распознавания спам-писем в качестве такого функционала может выступать доля неправильных ответов (предсказаний). Задача обучения заключается в подборе такого алгоритма, при котором достигается минимум функционала ошибки $Q(a, X)\\rightarrow min.$\n",
    "\n",
    "Наиболее подходящий алгоритм при этом выбирается из множества, называемого _семейством алгоритмов_ $\\mathbb{A}$. Их мы будем рассматривать в данном курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCfFle3N9INA"
   },
   "source": [
    "## 1. Линейная регрессия. MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLvKfPfN9INB"
   },
   "source": [
    "В предыдущем разделе мы упоминали линейные модели - это такие модели, которые сводятся к суммированию значений признаков с некоторыми весами. Само название модели говорит о том, что зависимость предсказываемой переменной от признаков будет линейной:\n",
    "\n",
    "$$a(x) = w_{0}+\\sum^{d}_{i=1}w_{i}x^{i}.$$\n",
    "\n",
    "В данном случае парамертрами моделей являются веса $w_{i}$. Вес $w_{0}$ называется _свободным коэффициентом_ или _сдвигом_. Оптимизация модели в таком случае заключается в подборе оптимальных значений весов. Сумму в формуле также можно описать как скалярное произведение вектора признаков $x=(x^{1},...,x^{d})$ на вектор весов $w=(w_{1},...,w_{d})$:\n",
    "\n",
    "$$a(x) = w_{0}+\\left \\langle w,x \\right \\rangle.$$\n",
    "\n",
    "Обратим внимание, что сдвиг делает модель неоднородной и затрудняет ее дальнейшую оптимизацию. Для устранения этого фактора обычно используют прием, позволяющий упростить запись: к признаковому описанию объекта добавляется еще один признак (константный), на каждом объекте равный единице. В этом случае вес при нем как раз будет по смыслу совпадать со свободным коэффициентом, и сам $w_{0}$ будет не нужен. Тогда получим\n",
    "\n",
    "$$a(x) = \\sum^{d+1}_{i=1}w_{i}x^{i}=\\left \\langle w,x \\right \\rangle.$$\n",
    "\n",
    "За счет простой формы линейные модели достаточно легко обучются и позволяют работать с зашумленными данными, небольшими выборками, контролирауя при этом риск переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8v9_4TB9INC"
   },
   "source": [
    "Для обучения модели необходимо иметь возможность измерять точность линейного алгоритма на выборке (обучающей или тестовой). \n",
    "\n",
    "В качестве меры ошибки можно взять абсолютное отклонение истинного значения от прогноза $Q(a,y)=a(x)-y$, но тогда минимизация функционала ошибки (в которой и состоит задача обучения) будет достигаться при принятии им отрицательного значения. Например, если истинное значение ответа равно $10$, а алгоритм $a(x)$ выдает ответ $11$, отклонение будет равно $1$, а при значении предсказания равном $1$, отклонение будет равно $1-10=-9$. Значение ошибки во втором случае ниже, однако разница истинного значения и предсказания нашей модели больше. Таким образом, такой функционал ошибки не поддается минимизации. \n",
    "\n",
    "Логичным кажется решение использовать в качестве функционала ошибки модуль отклонения $Q(a,y)=|a(x)-y|$. Соответствующий функционал ошибки называется средним абсолютным отклонением (mean absolute error, MAE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}|a(x_{i})-y_{i}|.$$\n",
    "\n",
    "Однако, как мы далее увидим, зачастую методы оптимизации включают в себя дифференцирование, а функция модуля не является гладкой - она не имеет производной в нуле, поэтому ее оптимизация бывает затруднительной.\n",
    "\n",
    "Поэтому сейчас основной способ измерить отклонение - посчитать квадрат разности $Q(a,y)=(a(x)-y)^{2}$. Такая функция является гладкой и имеет производную в каждой точке, а ее минимум достигается при равенстве истинного ответа $y$ и прогноза $a(x)$.\n",
    "\n",
    "Основанный на этой функции функционал ошибки называется _среднеквадратичным отклонением_ (mean squared error, MSE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}(a(x_{i})-y_{i})^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRwG-e0m9IND"
   },
   "source": [
    "## 2. Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpzXCWHG9INE"
   },
   "source": [
    "Как уже говорилось ранее, обучение модели регрессии заключается в минимизации функционала ошибки. Таким образом, в случае использования среднеквадратичной ошибки получаем задачу оптимизации\n",
    "\n",
    "$$Q(w,x) = \\frac{1}{l}\\sum^{l}_{i=1}(\\left \\langle w,x_{i} \\right \\rangle-y_{i})^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "Способ вычисления весов путем минимизации среднеквадратичного отклонения называется _методом наименьших квадратов_.\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eC50nWZ69INE"
   },
   "source": [
    "Имеет смысл переписать имеющиеся соотношения в матричном виде. В матрицу \"объекты-признаки\" впишем по строкам $d$ признаков для всех $l$ объектов из обучающей выборки: \n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "x_{11} & ... & x_{1d}\\\\ \n",
    "... & ... & ...\\\\ \n",
    "x_{l1} & ... & x_{ld}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "и составим вектор ответов $y$ из истинных ответов для данной выборки:\n",
    "\n",
    "$$y = \\begin{pmatrix}\n",
    "y_{1}\\\\ \n",
    "...\\\\ \n",
    "y_{l}\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "Помня, что $w$ - вектор параметров, переписанная в матричном виде задача будет выглядеть следующим образом:\n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}},$$\n",
    "\n",
    "где используется евклидова ($L_{2}$) норма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eW4aV609ING"
   },
   "source": [
    "Продифференцировав данную функцию по вектору $w$ и приравняв к нулю, можно получить явную анатилическую формулу для решения задачи минимизации (ссылка на подробный вывод формулы есть в списке дополнительных материалов):\n",
    "\n",
    "$$w = (X^{T}X)^{-1}X^{T}y.$$\n",
    "\n",
    "Это решение называется _нормальным уравнением_ линейной регрессии. Наличие аналитического решения кажется положительным фактором, однако, у него есть некоторые минусы, среди которых вычислительная сложность операции (обращение матрицы $X^{T}X$ будет иметь кубическую сложность от количества признаков $d^{3}$), а также тот факт, что матрица $X^{T}X$ может быть вырожденной и поэтому необратимой. Тогда найти решение будет невозможно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C63Alg-A9INH"
   },
   "source": [
    "Более удобным подходом будет разработка решения с помощью численных методов оптимизации, одним из которых является _градиентный спуск_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfRgH2cC9INI"
   },
   "source": [
    "## 3. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgHZFo-x9INK"
   },
   "source": [
    "Среднеквадратичная ошибка имеет один минимум и непрерывна на всей области значений (то есть является выпуклой и гладкой), а значит в каждой ее точке можно посчитать частные производные.\n",
    "\n",
    "Вспомним, что _градиентом_ функции $f$ называется $n$-мерный вектор из частных производных. \n",
    "\n",
    "$$ \\nabla f(x_{1},...,x_{d}) = \\left(\\frac{\\partial f}{\\partial x_{i}}\\right)^{d}_{i=1}.$$\n",
    "\n",
    "При этом известно, что __градиент задает направление наискорейшего роста функции__. Значит, антиградиент будет показывать направление ее скорейшего убывания, что будет полезно нам в нашей задаче минимизации функционала ошибки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EITeLDLk9INM"
   },
   "source": [
    "Для решения задачи нам требуется определить некоторую стартовую точку и итерационно сдвигаться от нее в сторону антиградиента с определенным _шагом_ $\\eta_{k}$, на каждом шагу пересчитывая градиент в точке, в которой мы находимся. Таким образом, имея начальный вектор весов $w^{0}$, $k$-й шаг градиентного спуска будет иметь вид\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD9YpghL9INO"
   },
   "source": [
    "Итерации следует продолжать, пока не наступает сходимость. Она определяется разными способами, но в даннном случае удобно определять как ситуацию, когда векторы весов от шага к шагу изменяются незначительно, то есть норма отклонения вектора весов на текущем шаге от предыдущего не привышает заданное значение $\\varepsilon$:\n",
    "\n",
    "$$||w^{k}-w^{k-1}|| < \\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4yKBld_9INT"
   },
   "source": [
    "Начальный вектор весов $w_{0}$ также можно определять различными способами, обычно его берут нулевым или состоящим из случайных небольших чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhuJyO__9INU"
   },
   "source": [
    "В случае многомерной регрессии (при количестве признаков больше 1) при оптимизации функционала ошибки \n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}}$$\n",
    "\n",
    "формула вычисления градиента принимает вид\n",
    "\n",
    "$$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkskCJdb9INV"
   },
   "source": [
    "Смоделируем работу градиентного спуска при помощи Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJ1KA9cq9INW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-sYDyFj9INZ"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKUBgDlf9INe"
   },
   "outputs": [],
   "source": [
    "# Возьмем 2 признака и 1000 объектов\n",
    "n_features = 2\n",
    "n_objects = 1000\n",
    "\n",
    "# сгенерируем вектор истинных весов\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "# сгенерируем матрицу X, вычислим Y с добавлением случайного шума\n",
    "X = np.random.uniform(-7, 7, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 0.5, size=(n_objects))\n",
    "\n",
    "# возьмем нулевые начальные веса\n",
    "w = np.zeros(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ujy3MY_s9INj"
   },
   "outputs": [],
   "source": [
    "# реализуем функцию, определяющую среднеквадратичную ошибку\n",
    "def mserror(X, w, y_pred):\n",
    "    y = X.dot(w)\n",
    "    return (sum((y - y_pred)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeKE8moY9INm"
   },
   "source": [
    "Реализуем функцию, вычисляющую вектор весов по нормальному уравнению линейной регрессии, и применим ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpxGlgkD9INn",
    "outputId": "dfff8dca-6f99-4ebf-8921-c1fa156f0a05"
   },
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    return np.linalg.solve((X.T).dot(X), (X.T).dot(y))\n",
    "\n",
    "normal_eq_w = normal_equation(X, Y)\n",
    "print(f'В случае использования нормального уравнения функционал ошибки составляет {round(mserror(X, normal_eq_w, Y), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cqw7Fffj9IN0"
   },
   "source": [
    "Обучим линейную регрессию путем градиентного спуска и получим графики изменения весов и ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vnzIkxt9IN1",
    "outputId": "28f71598-e128-4adb-da62-1601401800f1"
   },
   "outputs": [],
   "source": [
    "# список векторов весов после каждой итерации\n",
    "w_list = [w.copy()]\n",
    "\n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# шаг градиентного спуска\n",
    "eta = 0.01\n",
    "\n",
    "# максимальное число итераций\n",
    "max_iter = 1e4\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-8\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "\n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    new_w = w - 2 * eta * np.dot(X.T, (np.dot(X, w) - Y)) / Y.shape[0]\n",
    "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(mserror(X, new_w, Y))\n",
    "    \n",
    "    iter_num += 1\n",
    "    w = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "\n",
    "print(f'В случае использования градиентного спуска функционал ошибки составляет {round(errors[-1], 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Afahfb849IN4",
    "outputId": "f1eb65a2-f819-49d3-8cb6-491d06b01d49"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение весов (красной точкой обозначены истинные веса, сгенерированные вначале)\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiTINBlK9IN7"
   },
   "source": [
    "После каждой итерации значения искомых весов приближаются к истинным, однако, не становятся им равны из-за шума, добавленного в вектор ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFzwe8pW9IN8",
    "outputId": "511c62c8-8ee3-40cc-ac09-be80875cc9ab"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение функционала ошибки\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.title('MSE')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kcz4wFx9IOB"
   },
   "source": [
    "Видно, что изменение монотонно и начинается с высокой точки, после определенного количества итераций выходя на асимптоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noIKD8ls9IOD"
   },
   "source": [
    "Очень важно при использовании метода градиентного спуска правильно подбирать шаг. Если длина шага будет слишком мала, то метод будет слишком медленно приближаться к правильному ответу, и потребуется очень большое количество итераций для достижения сходимости. Если же длина наоборот будет слишком большой, появится вероятность \"перепрыгивания\" алгоритма через минимум функции или вообще отсутствия сходимости градиентного спуска.\n",
    "\n",
    "Применяется методика использования переменного размера шага: на начальных этапах берется большой шаг, который с увеличением количества итераций снижается. Одна из таких методик - вычисление на каждой итерации размера шага по формуле\n",
    "\n",
    "$$\\eta_{k} = \\frac{c}{k},$$\n",
    "\n",
    "где $c$ - некоторая константа, а $k$ - номер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E77_BVG59IOH"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7td81sE9IOG"
   },
   "source": [
    "1. [Вывод аналитической формулы решения уравнения линейной регрессии](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) (см. пункт 1.2)\n",
    "2. [R — значит регрессия](https://habr.com/ru/post/350668/)\n",
    "3. [Математическое описание метода градиентного спуска](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "4. [Beginners Guide to Regression Analysis and Plot Interpretations](https://www.hackerearth.com/ru/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/)\n",
    "5. [Документация NumPy](https://docs.scipy.org/doc/numpy-1.16.0/reference/routines.html)\n",
    "6. [Базовые принципы машинного обучения на примере линейной регрессии](https://habr.com/ru/company/ods/blog/322076/)\n",
    "7. [Коэффициент детерминации (R-квадрат)](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E77_BVG59IOH"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Линейная регрессия - простой, но зачастую эффективный, способ приближать вещественную целевую переменную через линейную комбинацию признаков\n",
    "* Решение регрессии - МНК, можно решать аналитически, но на практике - градиентный спуск (GD, Gradient Descent)\n",
    "* MSE удобна для GD, так как дифференцируема\n",
    "* Максимальное качество GD достигается малыми шагами и большим кол-вом итераций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Какие допущения применяются к линейной регрессии?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Есть линейная зависимость между зависимой переменной и регрессорами.\n",
    "* Остатки нормально распределены с нулевым средним значением и независимы друг от друга.\n",
    "* Существует минимальная мультиколлинеарность между объясняющими переменными.\n",
    "* Гомоскедастичность (дисперсия вокруг линии регрессии одинакова для всех значений предикторной переменной).\n",
    "\n",
    "Если какие-то допущения не выполняются, мы можем преобразовать Х или У и рассчитать новую линию регрессии, для которой эти допущения удовлетворяются (например, использовать логарифм)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Возможно ли использовать функцию MAE для градиентного спуска, несмотря на то, что она не является гладкой?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, производную в нуле можно доопределить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Как готовить данные для применения модели линейной регрессии?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Учесть линейное предположение, преобразовав признаки для их линейной связи с целевой переменной (степень, экспонента, логарифм..)\n",
    "* Удалить шум/выбросы, т.к. линейная регрессия предполагает, что ваши входные и выходные переменные не шумят. \n",
    "* Снизить коллинеарность, вычислив попарные корреляции признаков и удалить наиболее скоррелированные.\n",
    "* Гауссовские (нормальные) распределения признаков/таргета, так прогнозы будут точнее.\n",
    "* Масштабирование признаков - стандартизация/нормализация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Каĸ проверить, что случайная величина распределена нормально?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Применить статистический ĸритерий (например, [Критерий Шапиро-Уилка](http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%A8%D0%B0%D0%BF%D0%B8%D1%80%D0%BE-%D0%A3%D0%B8%D0%BB%D0%BA%D0%B0), [Критерий хи-квадрат](http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D1%85%D0%B8-%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82) или какой-то [другой](https://ru.wikipedia.org/wiki/%D0%9A%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B8_%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8)).\n",
    "* Построить гистограмму в логарифмической шкале - будет парабола, т.ĸ. плотность вероятности нормального распределения задается формулой $PDF = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\; \\exp\\left(-\\frac{\\left(x-\\mu\\right)^2}{2\\sigma^2} \\right)$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
