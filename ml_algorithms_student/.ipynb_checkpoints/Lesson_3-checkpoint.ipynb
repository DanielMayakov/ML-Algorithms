{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVKnqZEmWjr0"
   },
   "source": [
    "# Урок 3. Классификация. Логистическая регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QxTQ6b1GWjr2"
   },
   "source": [
    "## Линейная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0zTMhamWjr3"
   },
   "source": [
    "До этого мы разговаривали о задачах регрессии, то есть о восстановлении непрерывной зависимости по имеющимся данным. Однако, это не единственный тип задач в машинном обучении. В этом уроке речь пойдет о задачах _классификации_. Это такие задачи, в которых объекты делятся на конечное количество классов, и целью обучения является получение модели, способной соотносить объекты к тому или иному классу. \n",
    "\n",
    "Простейшим случаем является _бинарная классификация_, то есть случай, когда у нас имеется два класса. Единственное отличие от линейной регресси здесь в том, что пространство ответов состоит из двух элементов, в нашем случае возьмем $\\mathbb{Y} = \\{-1,1\\}$, где -1 и 1 означают принадлежность к первому или второму классу, соответственно. Пример такой задачи упоминался на первом уроке, когда говорилось о распознавании спам-писем. В этом случае, -1 означало, что письмо не является спамом, а 1 - что является."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwXb9QzEWjr4"
   },
   "source": [
    "Как и в случае регрессии, в классификации можно использовать линейные модели. Это называется _линейной классификацией._ Линейные классификаторы устроены похожим на линейную регрессию образом, за одним лишь различием - для получения бинарных значений берется только знак от значения $a(x)$:\n",
    "\n",
    "$$a(x) = \\text{sign}\\left (w_{0}+\\sum^{d}_{i=1}w_{i}x^{i} \\right ).$$\n",
    "\n",
    "Аналогично линейной регрессии, после добавления константного признака формула имеет вид\n",
    "\n",
    "$$\\text{sign} \\left ( \\sum^{d+1}_{i=1}w_{i}x^{i} \\right ) = \\text{sign} \\left ( \\left \\langle w,x \\right \\rangle \\right ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BinRsFPdWjr4"
   },
   "source": [
    "Множество точек $\\left \\langle w,x \\right \\rangle = 0$ образует _гиперплоскость_ в пространстве признаков и делит его на две части. Объекты, расположенние по разные стороны от нее, относятся к разным классам.\n",
    "\n",
    "Стоит отметить, что для некоторого объекта $x$ расстояние до этой гиперплоскости будет равняться $\\frac{| \\left \\langle w,x \\right \\rangle |}{||w||}$, соответственно, при классификации нам важен не только знак скалярного произведения $\\left \\langle w,x \\right \\rangle$, но и его значение: чем выше оно, тем больше будет расстояние от объекта до разделяющей гиперплоскости, что будет означать, что алгоритм более уверен в отнесении объекта к данному классу. Это приводит нас к значению _отступа_ (Margin), который равен скалярному произведению вектора весов $w$ на вектор признаков $x$, умноженному на истинное значение ответа $y$, которое, как мы помним, принимает значения -1 и 1:\n",
    "\n",
    "$$M_{i}=y_{i}\\left \\langle w,x_{i} \\right \\rangle.$$\n",
    "\n",
    "Таким образом, если скалярное произведение отрицательно, и истинный ответ равен -1, отступ будет больше нуля. Если скалярное произведение положительно, и истинный ответ равен 1, отступ также будет положителен. То есть $M_{i}>0$, когда классификатор дает верный ответ, и $M_{i}<0$, когда классификатор ошибается. Отступ характеризует корректность ответа, а его абсолютное значение свидетельствует о расстоянии от разделяющей гиперплоскости, то есть о мере уверенности в ответе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqXJRE2bWjr5"
   },
   "source": [
    "### Функционал ошибки в линейной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZE9IJfpWjr6"
   },
   "source": [
    "Как и в случае линейно регрессии, для обучения алгоритма линейной классификации требуется измерять ошибку. По аналогии со средней абсолютной ошибкой (MAE) и среднеквадратичной ошибкой (MSE) в случае линейной классификации можно использовать естественный подход: так как возможных ответов конечное число, можно требовать полного совпадения предсказанного класса $a(x_{i})$ и истинного $y_{i}$. Тогда в качестве функционала ошибки можно использовать долю неправильных ответов:\n",
    "\n",
    "$$Q(a, X) = \\frac{1}{l}\\sum^{l}_{i=1}[a(x_{i}) \\neq y_{i})]$$\n",
    "\n",
    "или, используя понятие отступа,\n",
    "\n",
    "$$Q(a, X) = \\frac{1}{l}\\sum^{l}_{i=1}[M_{i}<0] = \\frac{1}{l}\\sum^{l}_{i=1}[y_{i}\\left \\langle w,x_{i} \\right \\rangle < 0)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X33NjwGXWjr6"
   },
   "source": [
    "Функция, стоящая под знаком суммы, называется _функцией потерь_. График ее в зависимости от отступа будет иметь пороговый вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sg7z9JsRWjr7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52JJBg6lWjr-"
   },
   "outputs": [],
   "source": [
    "def loss_function(x):\n",
    "    return 0 if x > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTNFAk4pWjsB",
    "outputId": "f692f383-c883-4f73-b66f-94a95b3e9b73"
   },
   "outputs": [],
   "source": [
    "dots = np.linspace(-1, 1, 1000)\n",
    "q_zero_one_loss = [loss_function(x) for x in dots]\n",
    "\n",
    "plt.xlabel('Mi')\n",
    "plt.xlim(-1, 1)\n",
    "plt.plot(dots, q_zero_one_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxOij1O9WjsE"
   },
   "source": [
    "Она называется _пороговой функцией потерь_ или 1/0 функцией потерь. Как мы видим, она негладкая, поэтому градинентные методы оптимизации к ней неприменимы. Для упрощения оптимизации используют гладкие оценки сверху этой функции, то есть такие функции, что \n",
    "\n",
    "$$[M_{i}<0] \\leq \\tilde{L}(M_{i}).$$\n",
    "\n",
    "Тогда минимизировать уже нужно эту новую функцию:\n",
    "\n",
    "$$Q(a, X) \\leq \\tilde Q(a, X) = \\frac{1}{l}\\sum^{l}_{i=1}\\tilde{L}(M_{i}) \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "Примерами могут быть:\n",
    "\n",
    "- _экспоненциальная функция потерь_ $\\tilde{L}(M_{i}) = \\text{exp}(- M_{i})$\n",
    "\n",
    "\n",
    "- _квадратичная функция потерь_ $\\tilde{L}(M_{i}) = (1- (M_{i}))^{2}$\n",
    "\n",
    "\n",
    "- _логистическая функция потерь_ $\\tilde{L}(M_{i}) = \\text{log}_{2}(1 + \\text{exp}(- M_{i}))$\n",
    "\n",
    "\n",
    "- и др. (см. доп. материалы)\n",
    "\n",
    "Реализуем их и построим соответствующие графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3tpAnEQWjsF"
   },
   "outputs": [],
   "source": [
    "def exp_loss_func(x):\n",
    "    return np.exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GV0RcxPWjsH"
   },
   "outputs": [],
   "source": [
    "def square_loss(x):\n",
    "    return (1 - x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQQp4gx6WjsK"
   },
   "outputs": [],
   "source": [
    "def logistic_loss(x):\n",
    "    return np.log2(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XE8E9QqWjsM",
    "outputId": "07ad0628-87eb-430d-fe05-3e7fe572537e"
   },
   "outputs": [],
   "source": [
    "q_exp_loss = [exp_loss_func(x) for x in dots]\n",
    "q_logistic_loss = [logistic_loss(x) for x in dots]\n",
    "q_square_loss = [square_loss(x) for x in dots]\n",
    "\n",
    "plt.xlabel('Mi')\n",
    "plt.xlim(-1, 1)\n",
    "plt.plot(dots, q_zero_one_loss)\n",
    "plt.plot(dots, q_exp_loss)\n",
    "plt.plot(dots, q_square_loss)\n",
    "plt.plot(dots, q_logistic_loss)\n",
    "plt.legend(['zero-one loss', 'exponential loss', 'square loss', 'logistic loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tv0zYuyqWjsQ"
   },
   "source": [
    "Все они оценивают функцию потерь сверху и при этом хорошо оптимизируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5MlNOS7WjsR"
   },
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Urh_FsvlWjsS"
   },
   "source": [
    "_Логистическая регрессия_ - частный случай линейного классификатора, обладающий одной полезной особенностью - помимо отнесения объекта к определенному классу она умеет прогнозировать вероятность $P$ того, что объект относится к этому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IZd3LR9WjsT"
   },
   "source": [
    "Во многих задачах такая особенность является очень важной. Например, в задачах кредитного скоринга (предсказание, вернет клиент кредит или нет) прогнозируют вероятность невозврата кредита и на основании нее принимают решение о выдаче или невыдаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhYaDtqMWjsT"
   },
   "source": [
    "Пусть в каждой точке пространства объектов $\\mathbb{X}$ задана вероятность того, что объект $x$ будет принадлежать к классу \"+1\" $P(y=1|x)$ (условная вероятность $y = 1$ при условии $x$). Она будет принимать значения от 0 до 1, и нам нужно каким-то образом ее предсказывать, но пока мы умеем только строить прогноз методами линейной регрессии с помощью некоего алгоритма $b(x)=\\left \\langle w,x_{i} \\right \\rangle$. У него есть проблема, связанная с тем, что скалярное произведение $\\left \\langle w,x_{i} \\right \\rangle$ не всегда возвращает значения в отрезке [0, 1]. Чтобы достичь такого условия, можно использовать некую функцию $\\sigma:\\mathbb{R} \\rightarrow [0,1]$, которая будет переводить полученное в скалярном произведении значение в вероятность, пределы которой будут лежать в промежутке от 0 до 1. В модели логистической регрессии в качестве такой функции берется сигмоида, которая имеет вид:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + exp(-z)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqjAuOdgWjsU"
   },
   "source": [
    "Изобразим ее типичный график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MojlVOSyWjsV"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCAPY32dWjsX",
    "outputId": "6e6c58cf-a5fe-4845-cceb-6abd78cfb36c"
   },
   "outputs": [],
   "source": [
    "dots = np.linspace(-10, 10, 100)\n",
    "sigmoid_value = list(map(sigmoid, dots))\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.grid()\n",
    "plt.plot(dots, sigmoid_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmmM4TpFWjsZ"
   },
   "source": [
    "При использовании такой функции $\\tilde{b}(x_{i}) = \\sigma(\\left \\langle w,x_{i} \\right \\rangle)$ получаем, что вероятность отнесения объекта к классу \"+1\" $P(y=1|x)$, которую для краткости обозначим $p_{+}$, будет равняться\n",
    "\n",
    "$$p_{+} = \\sigma(\\left \\langle w,x_{i} \\right \\rangle) = \\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)},$$\n",
    "\n",
    "Чем больше будет скалярное произведение $\\left \\langle w,x_{i} \\right \\rangle$, тем выше будет предсказанная вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BxRP1WYWjsa"
   },
   "source": [
    "Чтобы понять, как его интерпретировать, выведем его из формулы выше:\n",
    "\n",
    "$$\\left \\langle w,x_{i} \\right \\rangle = \\text{ln} \\frac{p_{+}}{1 - p_{+}}.$$\n",
    "\n",
    "Таким образом, скалярное произведение вектора весов на вектор признаков представляет собой логарифм отношения вероятностей того, что y = 1 к вероятности того, что y = -1. Выражение под логарифмом называется _риском_, а вместе с логарифмом это выражение называется _логитом_. Поэтому метод и называется логистической регрессией: мы приближаем логит линейной комбинацией признаков и весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OR_0UT1sWjsb"
   },
   "source": [
    "Далее для обучения этой модели нам потребуется использовать _метод максимального правдоподобия_ (см. доп. материалы). Его сущность заключается в выборе гипотезы, при которой вероятность получить имеющееся наблюдение максимальна.\n",
    "\n",
    "С точки зрения реализуемого алгоритма вероятность того, что в выборке встретится объект $x_{i}$ c классом $y_{i}$, равна\n",
    "\n",
    "$$P(y=y_{i}|x_{i}) = p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$\n",
    "\n",
    "Исходя из этого, правдоподобие выборки (т.е. вероятность получить такую выборку с точки зрения алгоритма) будет равняться произведению вероятностей получения каждого имеющегося ответа:\n",
    "\n",
    "$$P(y|X) = L(X) = \\prod^{l}_{i=1} p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$\n",
    "\n",
    "Правдоподобие можно использовать как функционал для обучения алгоритма, однако, удобнее взять от него логарифм, так как в этом случае произведение превратится в сумму, а сумму гораздо проще оптимизировать. Также, в отличие от рассмотренных ранее функций потерь, правдоподобие требуется максимизировать для обучения алгоритма, а не минимизировать. Поэтому для большего удобства перед правдоподобием ставят минус, поскольку функции потери в задачах регрессии принято минимизировать. В итоге получим:\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1}([y_{i} = +1] \\text{ln}p_{+} + [y_{i} = -1]\\text{ln}(1 - p_{+})).$$\n",
    "\n",
    "Данная функция потерь называется _логарифмической функцией потерь (log loss)_ или _кросс-энтропией_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ55IxJ5Wjsc"
   },
   "source": [
    "Если мы подставим в нее полученное ранее выражение для $p_{+}$ для сигмоиды, получим:\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1}([y_{i} = +1] \\text{ln}\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)} + [y_{i} = -1]\\text{ln}(1 - \\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)})) =$$\n",
    "\n",
    "$$=\\sum^{l}_{i=1} \\text{ln}(1 + exp(-y_i\\left \\langle w,x_{i} \\right \\rangle))$$\n",
    "\n",
    "В случае, когда имеются классы 1 и -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2rGHbQkWjsd"
   },
   "source": [
    "То есть в случае логистической регрессии обучение сводится к минимизации этого функционала. Обратите внимание, что мы получили функцию, аналогичную показанной ранее логистической функции потерь, с точность до коэффициента $\\frac{1}{ln(2)}$, так как выражение в скобках как раз представляет собой отступ $M_{i} = -y_i\\left \\langle w,x_{i} \\right \\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiILjPR9Wjse"
   },
   "source": [
    "В случае, когда $y$ принимает значения 0 и 1, log loss запишется как\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1} (y_{i} \\text{ln}\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)} + (1 - y_{i})\\text{ln} (1-\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)})).$$\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1} (y_{i} \\text{ln}(\\sigma) + (1 - y_{i})\\text{ln} (1-\\sigma)).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rx8q4AbEWjsf"
   },
   "source": [
    "### Реализация логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRslRTEAWjsg"
   },
   "source": [
    "Напишем алгоритм логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6sxbQYJWjsh",
    "outputId": "fefee5f1-a278-4257-fb4b-03138e12c74a"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# сгеренируем данные с помощью sklearn.datasets\n",
    "classes = datasets.make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                                       n_redundant=0, n_classes=2, random_state=1)\n",
    "# datasets.make_blobs(centers = 2, cluster_std = 1.5, random_state=12)\n",
    "\n",
    "# и изобразим их на графике\n",
    "colors = ListedColormap(['red', 'blue'])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter([x[0] for x in classes[0]], [x[1] for x in classes[0]], c=classes[1], cmap=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0wmCApKWjsj"
   },
   "source": [
    "Далее разделим выборку на обучающую и тестовую. При реальной работе, если нет специфических требований по сохранению порядка выборки, ее полезно перемешивать, так как данные в ней могут быть каким-либо образом отсортированы. Это может негативно сказаться на процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Grk26bHWjsk",
    "outputId": "f002dc93-3214-444e-a921-0f73b27d8f70"
   },
   "outputs": [],
   "source": [
    "# перемешивание датасета\n",
    "np.random.seed(12)\n",
    "shuffle_index = np.random.permutation(classes[0].shape[0])\n",
    "X_shuffled, y_shuffled = classes[0][shuffle_index], classes[1][shuffle_index]\n",
    "\n",
    "# разбивка на обучающую и тестовую выборки\n",
    "train_proportion = 0.7\n",
    "train_test_cut = int(len(classes[0]) * train_proportion)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X_shuffled[:train_test_cut], \\\n",
    "    X_shuffled[train_test_cut:], \\\n",
    "    y_shuffled[:train_test_cut], \\\n",
    "    y_shuffled[train_test_cut:]\n",
    "    \n",
    "print(\"Размер массива признаков обучающей выборки\", X_train.shape)\n",
    "print(\"Размер массива признаков тестовой выборки\", X_test.shape)\n",
    "print(\"Размер массива ответов для обучающей выборки\", y_train.shape)\n",
    "print(\"Размер массива ответов для тестовой выборки\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUotFcKaWjsm"
   },
   "source": [
    "Далее транспонируем матрицы данных, так как нам удобнее работать со строками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzHyJ_vBWjsn"
   },
   "outputs": [],
   "source": [
    "X_train_tr = X_train.transpose()\n",
    "y_train_tr = y_train.reshape(1, y_train.shape[0])\n",
    "X_test_tr = X_test.transpose()\n",
    "y_test_tr = y_test.reshape(1, y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWeymQIWWjsq"
   },
   "source": [
    "Реализуем функцию потерь log loss с одновременным расчетом градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xw1hB380Wjsr"
   },
   "source": [
    "Оптимизировать функционал ошибки будем с помощью градиентного спуска, его вид в случае использования такой функции потерь будет:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\eta \\frac{1}{l}X(A-Y)^{T},$$\n",
    "\n",
    "где $A=\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhyYS76iWjss"
   },
   "outputs": [],
   "source": [
    "def log_loss(w, X, y):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # используем функцию сигмоиды, написанную ранее\n",
    "    A = sigmoid(np.dot(w.T, X))\n",
    "    \n",
    "    loss = -1.0 / m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "    loss = np.squeeze(loss)\n",
    "    grad = 1.0 / m * np.dot(X, (A - y).T)\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zChpWQBVWjsx"
   },
   "source": [
    "Реализуем градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwgPL740Wjsy"
   },
   "outputs": [],
   "source": [
    "def optimize(w, X, y, n_iterations, eta):\n",
    "#     потери будем записывать в список для отображения в виде графика\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_iterations):        \n",
    "        loss, grad = log_loss(w, X, y)\n",
    "        w = w - eta * grad\n",
    "\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYXIC_XoWjs0"
   },
   "source": [
    "и функцию для выполнения предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEQzLyXSWjs1"
   },
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    y_predicted = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X))\n",
    "    \n",
    "#     За порог отнесения к тому или иному классу примем вероятность 0.5\n",
    "    for i in range(A.shape[1]):\n",
    "        if (A[:,i] > 0.5): \n",
    "            y_predicted[:, i] = 1\n",
    "        elif (A[:,i] <= 0.5):\n",
    "            y_predicted[:, i] = 0\n",
    "    \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2e3t2NZWjs3",
    "outputId": "11936aa4-3396-4ded-c955-a6c36f5ad9b1"
   },
   "outputs": [],
   "source": [
    "# иницилизируем начальный вектор весов\n",
    "w0 = np.zeros((X_train_tr.shape[0], 1))\n",
    "\n",
    "n_iterations = 1000\n",
    "eta = 0.05\n",
    "\n",
    "w, losses = optimize(w0, X_train_tr, y_train_tr, n_iterations, eta)\n",
    "\n",
    "y_predicted_test = predict(w, X_test_tr)\n",
    "y_predicted_train = predict(w, X_train_tr)\n",
    "\n",
    "# В качестве меры точности возьмем долю правильных ответов\n",
    "train_accuracy = 100.0 - np.mean(np.abs(y_predicted_train - y_train_tr)*100.0)\n",
    "test_accuracy = 100.0 - np.mean(np.abs(y_predicted_test-y_test_tr)*100.0)\n",
    "\n",
    "print(f\"Итоговый вектор весов w: {w}\")\n",
    "print(f\"Точность на обучающей выборке: {train_accuracy:.3f}\")\n",
    "print(f\"Точность на тестовой выборке: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVAOczz8Wjs5"
   },
   "source": [
    "Покажем, как менялась при этом функция потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DibJKnsfWjs6",
    "outputId": "e751ba3d-df6e-4727-ba16-b3ae3b39fbd3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Log loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EqYZ35XWjs8"
   },
   "source": [
    "## Оценка качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A53yI7qKWjs8"
   },
   "source": [
    "Как и в случае линейной регрессии, в задачах классификации требуется оценивать качество обученной модели. Для этого существует большое количество подходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx0akNl1Wjs9"
   },
   "source": [
    "Наиболее очевидным и простым способом является расчет _доли правильных ответов_:\n",
    "\n",
    "$$accuracy(a,x) = \\frac{1}{l} \\sum^{l}_{i=1}[a(x_{i})=y_{i}].$$\n",
    "\n",
    "Эта метрика, однако, имеет определенные недостатки:\n",
    "\n",
    "- Она может неадекватно работать на несбалансированных выборках, в которых объектов одного класса намного больше остальных: например, если у нас имеется выборка с 950 объектами класса +1 и 50 класса -1, обыкновенная константная модель классификатора, которая на всех объектах отдает ответ +1, будет иметь долю правильных ответов 0,95, при этом сам классификатор является абсолютно бесполезным. Методом борьбы с этим заключается в введении коэффициента $q_{0}$, равного доле объектов самого большого класса. Доля правильных ответов для корректных алгоритмов должна лежать в промежутке $[q_{0}, 1]$\n",
    "\n",
    "- Она не учитывает \"цены ошибок\". В некоторых прикладных задачах ошибки разного рода могут иметь разную важность. Например, если говорить о кредитном скоринге, при постановке задачи необходимо определить, какая ошибка будет хуже: выдать кредит \"плохому\" клиенту или не выдать \"хорошему\". При этом используемая метрика качества должна учитывать цены разных ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rz2yM298Wjs9"
   },
   "source": [
    "### Матрица ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08n8s3-UWjs-"
   },
   "source": [
    "Удобно представлять ответы в виде комбинации истинного ответа и ответа алгоритма. При этом получается так называемая _матрица ошибок_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO9WvB2RWjs_"
   },
   "source": [
    "|  <empty>   | $$y = +1$$ | $$y = -1$$ |\n",
    "--- | --- | ---\n",
    "| __$$a(x) = +1$$__  |   TP    |   FP   |\n",
    "| __$$a(x) = -1$$__ |   FN    |   TN   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-TJ-Dy2WjtA"
   },
   "source": [
    "В матрице сверху отложены истинные ответы, слева - ответы алгоритма. Когда алгоритм относит объект к классу \"+1\", говорят, что он _срабатывает_, а когда к \"-1\", - _пропускает_. Если алгоритм сработал (дал положительный ответ) и объект действительно относится к классу \"+1\", говорят, что имеет место верное срабатывание/верный положительный ответ (True Positive, TP), а если объект не относится к классу \"+1\", это ложное срабатывание (False Positive, FP). Если алгоритм пропускает объект, а его истинный класс \"+1\", это ложный пропуск/ложный негативные ответ (False Negative, FN), а если истинный класс объекта \"-1\", имеет место истинный пропуск (True Negative, TN). При такой классификации уже есть два вида ошибок - ложные срабатывания и ложные пропуски. По главной диагонали в матрице ошибок располагаются верные ответы, по побочной - неверные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoL4ZQJFWjtA"
   },
   "source": [
    "### Точность и полнота"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqEe7wphWjtB"
   },
   "source": [
    "В классификации часто используются две метрики - _точность_ и _полнота_.\n",
    "\n",
    "Точность (precision) представляет из себя долю истинных срабатываний от общего количества срабатываний. Она показывает, насколько можно доверять алгоритму классификации в случае срабатывания\n",
    "\n",
    "$$precision(a, X) = \\frac{TP}{TP+FP}.$$\n",
    "\n",
    "Полнота (recall) считается как доля объектов, истинно относящихся к классу \"+1\", которые алгоритм отнес к этому классу\n",
    "\n",
    "$$recall(a, X) = \\frac{TP}{TP+FN},$$\n",
    "\n",
    "здесь $TP+FN$ как раз будут вместе составлять весь список объектов класса \"+1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYnpf8r-WjtB"
   },
   "source": [
    "__Пример__\n",
    "\n",
    "Пусть у нас есть выборка из 100 объектов, из которых 50 относится к классу \"+1\" и 50 к классу \"-1\" и для этой работы с этой выборкой мы рассматриваем две модели: $a_{1}(x)$ с матрицей ошибок\n",
    "\n",
    "|  <empty>   | $$y = +1$$ | $$y = -1$$ |\n",
    "--- | --- | ---\n",
    "| __$$a_{1}(x) = +1$$__  |   40    |   10   |\n",
    "| __$$a_{1}(x) = -1$$__ |   10    |   40   |\n",
    "    \n",
    "\n",
    "и $a_{2}(x)$ с матрицей ошибок:\n",
    " \n",
    " \n",
    "|  <empty>   | $$y = +1$$ | $$y = -1$$ |\n",
    "--- | --- | ---\n",
    "| __$$a_{2}(x) = +1$$__  |   22    |   2   |\n",
    "| __$$a_{2}(x) = -1$$__ |   28    |   48   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzCiUj65WjtC"
   },
   "source": [
    "Для первого алгоритма \n",
    "\n",
    "$$preсision(a_{1}, X)=0.8$$\n",
    "$$recall(a_{1}, X)=0.8$$\n",
    "\n",
    "Для второго алгоритма\n",
    "\n",
    "$$preсision(a_{2}, X)=0.92$$\n",
    "$$recall(a_{2}, X)=0.44$$\n",
    "\n",
    "Как мы видим, точность второй модели очень высока, но при этом сильно снижена полнота. Поэтому нужно правильно формировать бизнес-требования к модели, какой именно показатель должен быть определяющим. Например, если в задаче кредитного скоринга банк ставит цель возврата 90% кредитов, задачей ставится максимизация полноты при условии точности не ниже 0.9. А если при распознавании спама стоит требование, например, распознавать 95% спам-писем, задача состоит в максимизации точности при условии полноты не ниже 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBf11-4WWjtD"
   },
   "source": [
    "Однако, такое ограничение есть не всегда, и в остальных случаях требуется максимизировать и полноту и точность. Есть различные варианты объединения их в одну метрику, одним из наиболее удобных из них является _F-мера_, которая представляет собой среднее гармоническое между точностью и полнотой\n",
    "\n",
    "$$F = \\frac{2 \\cdot precision \\cdot recall }{ presision + recall}.$$\n",
    "\n",
    "В отличие от, например, среднего арифметического, если хотя бы один из аргументов близок к нулю, то и среднее гармоническое будет близко к нулю. По сути, F-мера является сглаженной версией минимума из точности и полноты (см. графики)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kgiOPsYWjtE",
    "outputId": "dd914767-38e0-4d63-fbab-e8c85de3fae9"
   },
   "outputs": [],
   "source": [
    "A, B = np.meshgrid(np.linspace(0.01, 1, 100), np.linspace(0.01, 1, 100))\n",
    "\n",
    "min_levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        min_levels[i, j] = min([A[i, j], B[i, j]])\n",
    "        \n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title('Minimum')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.grid()\n",
    "plt.contour(A, B, min_levels, levels=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9__Kd-LIWjtG",
    "outputId": "cca420b5-7583-422f-ff20-bc30a9589593"
   },
   "outputs": [],
   "source": [
    "f_levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        f_levels[i, j] = 2 * A[i, j] * B[i, j] / (A[i, j] + B[i, j])\n",
    "\n",
    "plt.figure(figsize=(6, 6))        \n",
    "plt.title('F-score')\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.grid()\n",
    "plt.contour(A, B, f_levels, levels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fm_bFr4RWjtI"
   },
   "source": [
    "Существует также усовершенствованная версия F-меры $F_{\\beta}$:\n",
    "\n",
    "$$F_{\\beta} = (1 + \\beta^{2}) \\frac{precision \\cdot recall}{\\beta^{2} \\cdot precision + recall}.$$\n",
    "\n",
    "Параметр $\\beta$ здесь определяет вес точности в метрике. При $\\beta = 1$ это среднее гармоническое, умноженное на 2 (чтобы в случае $precision = 1$ и $recall = 1$ $F_{1} = 1$). Его изменение требуется, когда необходимо отдать приоритет точности или полноте, как это было показано в примерах ранее. Чтобы важнее была полнота, $\\beta$ должно быть меньше 1, чтобы важнее была точность - больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzt177cAWjtI"
   },
   "source": [
    "Итак, мы научились определять вероятность отнесения объекта к тому или иному классу и метрики, которые характеризуют качество работы алгоритма $a(x)=[b(x)>t]$, и теперь, чтобы конвертировать ее в бинарную метку (сделать выводы о принадлежности к классу), нужно определить значение порога вероятности $t$, при котором объект нужно относить к соответствующему классу. Естественным кажется вариант, при котором порог равен 0,5, но он не всегда оказывается оптимальным. Зачастую интерес представляет сам вещественнозначный алгоритм $b(x)$, а порог будет выбираться позже в зависимости от требований к точности и полноте. В таком случае появляется потребность в измерении качества семейства алгоритмов $a(x)=[b(x)>t]$ с различными $t$.\n",
    "\n",
    "Есть способы оценки модели в целом, не привязываясь к конкретному порогу. Первый из них основан на использовании _ROC-кривой_. Такая кривая строится в следующих координатах:\n",
    "\n",
    "по оси $x$ откладывается доля ложных срабатываний (False Positive Rate) - отношение числа ложных срабатываний к общему размеру отрицательного класса:\n",
    "\n",
    "$$FPR = \\frac{FP}{FP+TN}$$\n",
    "\n",
    "по оси $y$ откладывается доля верных срабатываний (True Positive Rate) - отношение числа верных срабатываний к размеру положительного класса:\n",
    "\n",
    "$$TPR = \\frac{TP}{TP+FN},$$\n",
    "\n",
    "то есть TPR по сути представляет из себя полноту, о которой мы говорили ранее.\n",
    "\n",
    "Точка на графике будет соответствовать конкретному классификатору с некоторым значением порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9QFBNdNWjtJ"
   },
   "source": [
    "В качестве примера возьмем выборку из семи объектов, которым алгоритм $b(x)$ присвоил оценки принадлежности к классу 1:\n",
    "\n",
    "| $$b(x)$$ | 0 | 0.1 | 0.2 | 0.3 | 0.5 | 0.6 |\n",
    "| -------- | - | --- | --- | --- | --- | --- |\n",
    "|  $$y$$   | 0 |  0  |  1  |  1  |  0  |  1  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj9Dpt69WjtL"
   },
   "source": [
    "Теперь пойдем по порядку справа налево:\n",
    "\n",
    "1. Сначала выбираем самый большой порог, при котором ни один объект не будет отнесен к первому классу. При этом доля верных срабатываний и доля ложных срабатываний равны нулю. Получаем точку (0, 0).\n",
    "2. Далее снижая порог до 0,6, один объект будет отнесен к первому классу. Доля ложных срабатываний останется нулевой, доля верных срабатываний станет 1/3.\n",
    "3. При дальнейшем уменьшении порога до 0,5 второй справа один объект будет отнесен к первому классу. TPR останется 1/3, FPR также станет равна 1/3.\n",
    "4. Далее при снижении порога до 0.3 TPR станет 2/3, FPR останется 1/3.\n",
    "5. При пороге 0.2 TPR станет равна 1, FPR останется 1/3.\n",
    "6. При пороге 0.2 5 объектов будут отнесены алгоритмом к классу 1, TPR останется 1, FPR станет 2/3.\n",
    "7. При дальнейшем уменьшении порога все объекты будут отнесены к первому классу, и TPR и FPR станут равны 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HIG6RE7WjtM"
   },
   "source": [
    "Построим соответствующий график"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msxJ90-UWjtN",
    "outputId": "a7d41da5-e2ab-4b6b-9937-1cfa12a62af5"
   },
   "outputs": [],
   "source": [
    "from numpy import trapz  # используем эту функцию для расчета площади под кривой\n",
    "\n",
    "TPR = [0, 0.33, 0.33, 0.66, 1, 1, 1]\n",
    "FPR = [0, 0, 0.33, 0.33, 0.33, 0.66, 1]\n",
    "\n",
    "AUC_ROC = trapz(TPR, x = FPR, dx=0.1)\n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.grid()\n",
    "plt.legend(' ', title=f'AUC-ROC={AUC_ROC:.3f}', loc='lower right')\n",
    "plt.plot(FPR, TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyESKowjWjtP"
   },
   "source": [
    "ROC кривая всегда идет из точки (0,0) в точку (1,1). При этом в случае наличия идеального классификатора с определенным порогом доля его верных ответов будет равна 1, а доля ложных срабатываний - 0, то есть график будет проходить через точку (0,1). Таким образом, чем ближе к этой точке проходит ROC-кривая, тем лучше наши оценки и лучше используемое семейство алгоритмов. Таким образом мерой качества оценок принадлежности к классу 1 может служить площадь под ROC-кривой. Такая метрика называется AUC-ROC (Area Under Curve - площадь под кривой ROC). В случае идеального алгоритма $AUC-ROC = 1$, а в случае худшего приближается к $\\frac{1}{2}$.\n",
    "\n",
    "Критерий AUC-ROC можно интерпретировать как вероятность того, что если выбрать случайные положительный и отрицательный объект выборки, положительный объект получит оценку принадлежности выше, чем отрицательный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsKR7UVvWjtP"
   },
   "source": [
    "Обычно объектов гораздо больше, чем в нашем примере, поэтому кривая в реальных задачах выглядит несколько иначе - в ней больше точек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcpsbtjDWjtQ"
   },
   "source": [
    "AUC-ROC не очень устойчив к несбалансированным выборкам. Допустим, нам нужно выбрать 100 релевантных документов из выборки в 1000000 документов. И у нас есть алгоритм, который дает выборку из 5000 документов, 90 из которых релевантны. В этом случае \n",
    "\n",
    "$$TPR=\\frac{TP}{TP+FN} = \\frac{90}{90+10}=0.9$$\n",
    "\n",
    "$$FPR=\\frac{FP}{FP+TN} = \\frac{4910}{4910+994990}=0.00491$$,\n",
    "\n",
    "Что является показателями очень хорошего алгоритма - AUC-ROC будет близка к 1, хотя на самом деле 1900 из 2000 выданных документов являются нерелевантными.\n",
    "\n",
    "Чтобы посмотреть реальное положение дел, рассчитаем точность и полноту:\n",
    "\n",
    "$$precision = \\frac{TP}{TP+FP}=\\frac{90}{90+4910} = 0.018$$\n",
    "\n",
    "$$recall = TPR = 0.9.$$\n",
    "\n",
    "Здесь уже видно, что алгоритм является недостаточно точным.\n",
    "\n",
    "Таким образом, если размер положительного класс значительно меньше отрицательного, AUC-ROC может давать неадекватную оценку качества алгоритма, так как измеряет долю ложных срабатываний относительно общего числа отрицательных объектов, и если оно большое, доля будет мала, хотя в абсолютном значении количество ложных срабатываний может заметно превышать количество верных срабатываний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4r8uwAqDWjtR"
   },
   "source": [
    "Избавиться от такой проблемы можно используя другой метод - _кривую точности-полноты (PR-кривую)_. По оси $x$ откладывается полнота, по оси $y$ - точность, а точка на графике, аналогично ROC-кривой, будет соответствовать конкретному классификатору с некоторым значением порога. \n",
    "\n",
    "Возьмем использованный нами для постройки ROC-кривой набор данных и аналогичным образом построим PR-кривую.\n",
    "\n",
    "\n",
    "| $$b(x)$$ | 0 | 0.1 | 0.2 | 0.3 | 0.5 | 0.6 |\n",
    "| -------- | - | --- | --- | --- | --- | --- |\n",
    "|  $$y$$   | 0 |  0  |  1  |  1  |  0  |  1  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VcA_V-6WjtS",
    "outputId": "cd7e6ef6-9c96-4e71-e224-d7d5be406569"
   },
   "outputs": [],
   "source": [
    "precision = [0, 1, 0.5, 0.66, 0.75, 0.6, 0.5]\n",
    "recall = [0, 0.33, 0.33, 0.66, 1, 1, 1]\n",
    "\n",
    "AUC_PR = trapz(precision, x = recall, dx=0.1)\n",
    "\n",
    "plt.title('PR curve')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('presision')\n",
    "plt.grid()\n",
    "plt.legend(' ', title=f'AUC-PR={AUC_PR:.3f}', loc='lower right')\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waknResiWjtV"
   },
   "source": [
    "Она всегда стартует в точке (0,0) и заканчивается в точке (1, r), где r - доля положительных объектов в выборке. В случае наличия идеального классификатора, у которого точность и полнота 100%, кривая пройдет через точку (1,1). Таким образом, чем ближе к этой точке кривая проходит, тем лучше оценки. Так что, как и в случае ROC-кривой, можно ввести метрику качества в виде площади под PR-кривой AUC-PR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBQTgdl2WjtW"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4eqL1suWjtW"
   },
   "source": [
    "1. [Функции потерь для классификации](https://en.wikipedia.org/wiki/Loss_functions_for_classification)\n",
    "2. Метод максимального правдоподобия: [Сложное описание](https://habr.com/ru/company/ods/blog/323890/#metod-maksimalnogo-pravdopodobiya) / [Простое описание](https://www.youtube.com/watch?v=2iRIqkm1mug)\n",
    "3. [Встроенные датасеты Sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)\n",
    "4. [Площадь под кривой - numpy.trapz](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.trapz.html)\n",
    "5. [Байесовская теория классификации. Логистическая регрессия. Восстановление смеси плотностей](http://www.machinelearning.ru/wiki/images/archive/c/c6/20160426232921!Voron-ML-Bayes2-slides.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5wUbEYBWjtX"
   },
   "source": [
    "##  Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Логистическая регрессия - частный случай линейной классификации - предсказывает вероятность отнесения объекта к основному классу, что зачастую очень важно при интерпретации\n",
    "* Для \"отображения\" действительных предсказаний линейной модели в \"вероятностный\" интервал [0,1] применяют сигмоиду\n",
    "* Для обучения логистической регрессии используют логарифмическую функцию потерь (log-loss), полученную методом максимального правдоподобия (maximum likelihood estimation)\n",
    "* Оптимизируем log-loss классическим градиентным спуском, в котором берем градиент log-loss'а\n",
    "* Основными метриками качества классификатора являются Accuracy, Precision, Recall, ROC-AUC, PR-AUC\n",
    "* Нужно быть внимательным при работе с этими метриками и хорошо понимать, как они работают и как между собой связаны, иначе выводы могут получиться неверными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Оптимальные веса для логистической регрессии можно искать только градиентным спуском или есть еще какие-то оптимизационные алгоритмы?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск - это метод первого порядка, а есть методы второго порядка, например, __метод Ньютона — Рафсона__. Его применение приводит к методу наименьших квадратов (МНК) с итеративным пересчетом весов (англ IRLS, Iteratively Reweighted Least Squares). Обновление весов задается следующим рекурентным соотношением:\n",
    "\n",
    "$$w^{k+1} = w^{k} + \\eta_{k}(X^T\\Lambda X)^{-1}X^T\\tilde{y}$$ \n",
    "где \n",
    "$$\\tilde{y}=y_i(1-\\sigma_i)$$\n",
    "$$\\Lambda=diag((1-\\sigma_i)/\\sigma_i)$$\n",
    "$$\\sigma_i=\\sigma(y_iw^Tx_i)=P(y_i|x_i)$$\n",
    "Подробнее про метод Ньютона — Рафсона можно почитать в [примере решения задачи логистической регрессии](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F_%28%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%29), на [странице вики с методом Ньютона](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%9D%D1%8C%D1%8E%D1%82%D0%BE%D0%BD%D0%B0#%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%9D%D1%8C%D1%8E%D1%82%D0%BE%D0%BD%D0%B0_%E2%80%94_%D0%A0%D0%B0%D1%84%D1%81%D0%BE%D0%BD%D0%B0) и в [лекциях по алгоритмам восстановления регрессии](http://www.ccas.ru/voron/download/Regression.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Как векторно задается уравнение гиперплоскости, разделяющей примеры на 2 класса?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec{w}^T\\vec{x} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Чему равен ROC-AUC константного ответа 0 на выборке из 9900 объектов класса 0 и 100 объектов класса 1?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 0\n",
    "2. 0.01\n",
    "3. 0.49\n",
    "4. __0.5__\n",
    "5. 0.51\n",
    "6. 0.99\n",
    "7. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Предложите инвариантное преобразование вероятностей, чтобы ROC не изменилась.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабирование, например, нормализация (MinMaxScaler). При таком преобразовании порядок объектов, отсортированных по значению вероятности, не изменится, а значит ROC будет строиться так же."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Как интерпретировать следующую ситуацию: ROC-AUC близок к 1, а Precision <70%?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько вариантов:\n",
    "* Дизбаланс классов - положительных объектов (метка \"1\") много меньше, чем отрицательных (метка \"0\")\n",
    "* Выбран низкий порог вероятности, модель выдает больше \"1\", ошибок I рода больше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Может ли модель логистической регрессии идеально решить задачу классификации на таких данных?__\n",
    "\n",
    "<img src=\"images/L3_logit_example.jpg\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет, так как логистическая регрессия формирует линейную разделяющую плоскость (в двумерном случае - линию), а объекты на картинке не разделимы прямой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Два ĸласса идеально линейно разделимы, будет ли оптимум для ĸлассифиĸатора?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае линейно разделимой выборки не существует вектора весов, который бы максимизировал правдободобие вероятностной модели логической регрессии."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
