{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41AitfhKbsha"
   },
   "source": [
    "# Урок 6. Градиентный бустинг. AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOGOQj2gbshe"
   },
   "source": [
    "В этом уроке мы продолжаем тему ансамблей алгоритмов, рассматривая еще один их вид - _градиентный бустинг_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTW1uqTSbshf"
   },
   "source": [
    "Вспоминая тему предыдущего урока, случайные леса, напомним, что случайный лес - это ансамбль деревьев небольшой глубины, строящихся независимо друг от друга. В независимости построения деревьев кроется и плюс, и минус алгоритма: с одной стороны, построение деревьев можно распараллеливать и, например, организовывать на разных ядрах процессора, с другой стороны, следствием их независимости является тот факт, что для решения сложных задач требуется очень большое количество деревьев. В этих случаях случаях (при большой выборке или большом количестве признаков) обучение случайного леса может требовать очень много ресурсов, а если для ограничения их потребления слишком ограничивать глубину деревьев, они могут не уловить все закономерности в данных и иметь большой сдвиг (и, следовательно, ошибку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1y67OFwjbshg"
   },
   "source": [
    "Бустинг является своеобразным решением этой проблемы: он заключается в последовательном построении ансамбля, когда деревья строятся одно за другим, и при этом каждое следующее дерево строится таким образом, чтобы исправлять ошибки уже построенного на данный момент ансамбля. При таком подходе базовые алгоритмы могут быть достаточно простыми, то есть можно использовать неглубокие деревья."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrgYbxN2bshh"
   },
   "source": [
    "## Алгоритм градиентного бустинга (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76g2hO7kbshi"
   },
   "source": [
    "Итоговый алгоритм ищется в виде взвешенной суммы базовых алгоритмов (обратите внимание: не среднего, а суммы):\n",
    "\n",
    "$$a_{N}(x) = \\sum^{N}_{n=1}\\gamma_{n}b_{n}(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hrsZoI9bshj"
   },
   "source": [
    "В случае линейной регрессии задача состоит в минимизации среднеквадратичного функционала ошибки:\n",
    "\n",
    "$$\\frac{1}{l}\\sum_{i=1}^{l}(a(x_{i}) - y_{i})^{2} \\rightarrow \\text{min}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2nkwUaQbshj"
   },
   "source": [
    "Так как ансамбль строится итеративно, нужно вначале обучить первый простой алгоритм:\n",
    "\n",
    "$$b_{1}(x) = \\underset{b}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i}) - y_{i})^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ac6rmRhbshk"
   },
   "source": [
    "Как мы помним, такая задача легко решается методом градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8FfQZLgbshl"
   },
   "source": [
    "После того, как мы нашли первый алгоритм $b_{1}(x)$, нам нужно добавить в ансамбль еще один алгоритм $b_{2}(x)$. Для начала найдем разницу ответов первого алгоритма с реальными ответами:\n",
    "\n",
    "$$s_{i}^{(1)} = y_{i} - b_{1}(x_{i}).$$\n",
    "\n",
    "Если прибавить эти значения к полученным предсказаниям, получим идеальный ответ. Таким образом, новый алгоритм логично обучать так, чтобы его ответы были максимально близки к этой разнице, чтобы при их прибавлении к ответам первого алгоритма мы получили близкие к реальным. Значит, второй алгоритм будет обучаться на следующем функционале ошибки:\n",
    "\n",
    "$$b_{2}(x) = \\underset{b}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i}) - s_{i}^{(1)})^{2} = \\underset{b}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i}) - (y_{i} - b_{1}(x_{i})))^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqJFy1z5bshm"
   },
   "source": [
    "Каждый следующий алгоритм также настраивается на остатки композиции из предыдущих алгоритмов:\n",
    "\n",
    "$$b_{N}(x) = \\underset{b}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i}) - s_{i}^{(N)})^{2},$$ \n",
    "\n",
    "$$s_{i}^{(N)} = y_{i} - \\sum_{n=1}^{N-1}b_{n}(x_{i}) = y_{i} - a_{N-1}(x_{i}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjU9v-qObshn"
   },
   "source": [
    "Таким образом, каждый новый алгоритм корректирует ошибки предыдущих, и так продолжается до момента получения приемлемой ошибки на композиции. Вектор коэффициентов $s$ при этом называют _вектором сдвига_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvZvWsCUbsho"
   },
   "source": [
    "Выбор сдвига из условия $s_{i} = y_{i} - a_{N-1}(x_{i})$ требует точного совпадения полученных предсказаний и ответов, однако, в более общем случае вектор сдвига принимают с учетом особенностей используемой в данном случае функции потерь: вектор сдвига должен ее минимизировать, то есть направлять в сторону уменьшения. Как мы помним из метода градиентного спуска, направление наискорейшего убывания функции совпадает с ее антиградиентом. Таким образом, если при обучении мы минимизируем функционал ошибки $L(y,z)$\n",
    "\n",
    "$$\\sum_{i=1}^{l}L(y_{i}, a_{N-1}(x_{i}) + s_{i}) \\rightarrow \\underset{s}{\\text{min}},$$\n",
    "\n",
    "сдвиг на каждом шаге должен быть противоположен производной функции потерь в точке $z = a_{N-1}(x_{i})$. \n",
    "\n",
    "$$s_{i} = \\left.-\\frac{\\partial L}{\\partial z} \\right|_{z = a_{N-1}(x_{i})}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZEVla3Rbsho"
   },
   "source": [
    "Каждый новый алгоритм таким образом выбирается, чтобы как можно лучше приближать антиградиент ошибки на обучающей выборке. \n",
    "\n",
    "После того, как мы вычислили требуемый для минимизации ошибки сдвиг $s$, нужно настроить алгоритм $b_{N}(x)$ так, чтобы он давал максимально близкие к нему ответы, то есть обучать его именно на вектор сдвига. Близость ответов алгоритма к сдвигу обычно оценивается с помощью среднеквадратичной ошибки независимо от условий исхожной задачи (так как исходно используемая функция потерь $L$ уже учтена в сдвигах $s_{i}$):\n",
    "\n",
    "$$b_{N}(x) = \\underset{s}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i})-s_{i})^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPvJXnRebshp"
   },
   "source": [
    "Коэффициент $\\gamma$ для найденного алгоритма также находится по аналогии с наискорейшим градиентным спуском:\n",
    "\n",
    "$$\\gamma_{N} = \\underset{\\gamma}{\\text{argmin}}\\sum_{i=1}^{l}L(y_{i},a_{N-1}(x_{i}) + \\gamma b_{N}(x_{i})).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9d5iQMt3bshq"
   },
   "source": [
    "Обычно в качестве функции потерь в задачах регрессии принимается квадратичная функция потерь ($L_{2}$ loss):\n",
    "\n",
    "$$L(y, z) = (y-z)^{2},$$\n",
    "\n",
    "его производная по $z$ примет вид \n",
    "\n",
    "$$L'(y, z) = 2(z-y)$$\n",
    "\n",
    "или модуль отклонения ($L_{1}$ loss)\n",
    "\n",
    "$$L(y, z) = |y-z|,$$\n",
    "\n",
    "его производная по $z$ будет иметь вид \n",
    "\n",
    "$$L'(y, z) = \\text{sign}(z-y).$$\n",
    "\n",
    "В случае классификации - логистическая функция потерь:\n",
    "\n",
    "$$L(y, z) = log(1 + exp(-yz))$$\n",
    "\n",
    "ее производная:\n",
    "\n",
    "$$L'(y, z) = \\frac{y_{i}}{1+exp(-yz)}.$$\n",
    "\n",
    "Следует помнить, что компоненты $s_{i}$, вычисляемые через эти производные, берутся с минусом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaEoz2tAbshr"
   },
   "source": [
    "Аналогично алгоритму градиентного спуска, имеет смысл добавлять ответ каждого нового алгоритма не полностью, а с некоторым шагом $\\eta \\in (0, 1]$, так как базовые алгоритмы обычно достаточно простые (например, деревья малой глубины), и они могут плохо приближать вектор антиградиента, и тогда вместо приближения к минимуму мы будем получать случайное блуждание в пространстве. В градиентном бустинге такой прием называется сокращением шага.\n",
    "\n",
    "$$a_{N}(x) = a_{N-1}(x) + \\eta \\gamma_{N} b_{N}(x).$$\n",
    "\n",
    "Градиентный бустинг склонен к переобучению при увеличении числа итераций $N$ или глубины входящих в него деревьев. Стоит об этом помнить при построении алгоритма и выбирать оптимальные параметры по отложенной выборке или с помощью кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djO4wPtSbshr"
   },
   "source": [
    "В конечном итоге алгоритм построения модели градиентного бустинга заключается в следующих шагах:\n",
    "\n",
    "__1.__ Для инициализации выбирается произвольный простой алгоритм $b_{0}(x)$, в его роли можно брать обычные константные алгоритмы: в случае задачи регрессии это может быть\n",
    "\n",
    "$$b_{0}(x) = 0$$\n",
    "\n",
    "или среднее значение по всем объектам обучающей выборки \n",
    "\n",
    "$$b_{0}(x) = \\frac{1}{l}\\sum_{i=1}^{l}y_{i};$$\n",
    "\n",
    "в случае классификации - самый часто встречающийся в выборке класс\n",
    "\n",
    "$$b_{0}(x) = \\underset{y}{\\text{argmax}}\\sum_{i=1}^{l}[y_{i} = y].$$\n",
    "\n",
    "__2.__ Для каждой итерации вычисляется вектор сдвига $s$:\n",
    "\n",
    "$$s = \\left ( \\left.-\\frac{\\partial L}{\\partial z} \\right|_{z = a_{n-1}(x_{1})},...,\\left.-\\frac{\\partial L}{\\partial z} \\right|_{z = a_{n-1}(x_{l})}\\right );$$\n",
    "\n",
    "находится алгоритм\n",
    "\n",
    "$$b_{n}(x) = \\underset{s}{\\text{argmin}}\\frac{1}{l}\\sum_{i=1}^{l}(b(x_{i})-s_{i})^{2};$$\n",
    "\n",
    "находится оптимальный коэффициент $\\gamma$\n",
    "\n",
    "$$\\gamma_{n} = \\underset{\\gamma}{\\text{argmin}}\\sum_{i=1}^{l}L(y_{i},a_{n-1}(x_{i}) + \\gamma b_{n}(x_{i})).$$\n",
    "\n",
    "и добавляется в имеющийся ансамбль с умножением на шаг $\\eta$, называемый _скоростью обучения_ (shrinkage)\n",
    "\n",
    "$$a_{n}(x) = a_{n-1}(x) + \\eta \\gamma_{n} b_{n}(x).$$\n",
    "\n",
    "__3.__ При достижении критериев остановки компонуется итоговая модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rg5ilDbTbshs"
   },
   "source": [
    "### Стохастический градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJrobWhDbsht"
   },
   "source": [
    "Как и в случае с градиентым спуском, есть так называемый стохастический градиентный бустинг, являющийся упрощенной (в плане потребления ресурсов) версией алгоритма. Его суть заключается в обучении каждого нового базового алгоритма на новой итерации не на всей обучающей выборке, а на некоторой ее случайной подвыборке. Практика показывает, что такой алгоритм позволяет получить такую же ошибку или даже уменьшить ее при том же числе итераций, что и в случае использования обычного бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTdnsVCnbshu"
   },
   "source": [
    "## Реализация алгоритма градиентного бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ll5qHP2bshv"
   },
   "source": [
    "Реализуем средствами Python алгоритм градиентного бустинга для деревьев решений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sG8r2RPbshw"
   },
   "source": [
    "Реализация деревьев решений была дважды продемонстрирована в предыдущих уроках, в этом не будем ее повторять и возьмем готовую реализацию дерева решений для регрессии из библиотеки `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk-74OFhbshx"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYD1kLOibsh0"
   },
   "source": [
    "Используем один из \"игрушечных\" датасетов из той же библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDZbSvqMbsh1"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leu8bBI7bsh6"
   },
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cz_JhiIpbsh8"
   },
   "source": [
    "Разделим выборку на обучающую и тестовую в соотношении 75/25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExZPR9FLbsh9"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xJCdggZbsh_"
   },
   "source": [
    "Напишем функцию, реализующую предсказание в градиентном бустинге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU_Rkc63bsiA"
   },
   "outputs": [],
   "source": [
    "def gb_predict(X, trees_list, coef_list, eta):\n",
    "    # Реализуемый алгоритм градиентного бустинга будет инициализироваться нулевыми значениями,\n",
    "    # поэтому все деревья из списка trees_list уже являются дополнительными и при предсказании прибавляются с шагом eta\n",
    "    return np.array([sum([eta* coef * alg.predict([x])[0] for alg, coef in zip(trees_list, coef_list)]) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gZvsyfSbsiD"
   },
   "source": [
    "В качестве функционала ошибки будем использовать среднеквадратичную ошибку. Реализуем соответствующую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xbjFIEKbsiE"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_real, prediction):\n",
    "    return (sum((y_real - prediction)**2)) / len(y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qlr3KD6zbsiH"
   },
   "source": [
    "Используем $L_{2}$ loss $L(y, z) = (y-z)^{2},$ ее производная по $z$ примет вид $L'(y, z) = 2(z-y)$. Реализуем ее также в виде функции (коэффициент 2 можно отбросить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRaZEd3ebsiI"
   },
   "outputs": [],
   "source": [
    "def bias(y, z):\n",
    "    return (y - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ut-7dBgVbsiK"
   },
   "source": [
    "Реализуем функцию обучения градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoIdAoPYbsiL"
   },
   "outputs": [],
   "source": [
    "def gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta):\n",
    "    \n",
    "    # Деревья будем записывать в список\n",
    "    trees = []\n",
    "    \n",
    "    # Будем записывать ошибки на обучающей и тестовой выборке на каждой итерации в список\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "\n",
    "        # инициализируем бустинг начальным алгоритмом, возвращающим ноль, \n",
    "        # поэтому первый алгоритм просто обучаем на выборке и добавляем в список\n",
    "        if len(trees) == 0:\n",
    "            # обучаем первое дерево на обучающей выборке\n",
    "            tree.fit(X_train, y_train)\n",
    "            \n",
    "            train_errors.append(mean_squared_error(y_train, gb_predict(X_train, trees, coefs, eta)))\n",
    "            test_errors.append(mean_squared_error(y_test, gb_predict(X_test, trees, coefs, eta)))\n",
    "        else:\n",
    "            # Получим ответы на текущей композиции\n",
    "            target = gb_predict(X_train, trees, coefs, eta)\n",
    "            \n",
    "            # алгоритмы начиная со второго обучаем на сдвиг\n",
    "            tree.fit(X_train, bias(y_train, target))\n",
    "            \n",
    "            train_errors.append(mean_squared_error(y_train, gb_predict(X_train, trees, coefs, eta)))\n",
    "            test_errors.append(mean_squared_error(y_test, gb_predict(X_test, trees, coefs, eta)))\n",
    "\n",
    "        trees.append(tree)\n",
    "        \n",
    "    return trees, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vkHFhhzbsiN"
   },
   "source": [
    "Теперь обучим несколько моделей с разными параметрами и исследуем их поведение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WvxluSlbsiO"
   },
   "outputs": [],
   "source": [
    "# Число деревьев в ансамбле\n",
    "n_trees = 10\n",
    "\n",
    "# для простоты примем коэффициенты равными 1\n",
    "coefs = [1] * n_trees\n",
    "\n",
    "# Максимальная глубина деревьев\n",
    "max_depth = 3\n",
    "\n",
    "# Шаг\n",
    "eta = 1\n",
    "\n",
    "trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDMFn3R-bsiR"
   },
   "outputs": [],
   "source": [
    "def evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta):\n",
    "    train_prediction = gb_predict(X_train, trees, coefs, eta)\n",
    "\n",
    "    print(f'Ошибка алгоритма из {n_trees} деревьев глубиной {max_depth} \\\n",
    "    с шагом {eta} на тренировочной выборке: {mean_squared_error(y_train, train_prediction)}')\n",
    "\n",
    "    test_prediction = gb_predict(X_test, trees, coefs, eta)\n",
    "\n",
    "    print(f'Ошибка алгоритма из {n_trees} деревьев глубиной {max_depth} \\\n",
    "    с шагом {eta} на тестовой выборке: {mean_squared_error(y_test, test_prediction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEBbjCynbsiV",
    "outputId": "e3e7bf19-aa15-4340-eceb-e2799f36140c"
   },
   "outputs": [],
   "source": [
    "evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TluGn7Iubsib"
   },
   "source": [
    "Построим графики зависимости ошибки на обучающей и тестовой выборках от числа итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXrSdSgjbsic"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjDdKkgObsif"
   },
   "outputs": [],
   "source": [
    "def get_error_plot(n_trees, train_err, test_err):\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlim(0, n_trees)\n",
    "    plt.plot(list(range(n_trees)), train_err, label='train error')\n",
    "    plt.plot(list(range(n_trees)), test_err, label='test error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Was5hOJPbsih",
    "outputId": "e4a4837d-6d3e-4861-8a35-fa26d5746f62"
   },
   "outputs": [],
   "source": [
    "get_error_plot(n_trees, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NS16dUwRbsik"
   },
   "source": [
    "Такой результат не является удовлетворительным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9n8R32khbsil"
   },
   "source": [
    "Увеличим число деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdiTyDJrbsil"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_trees = 50\n",
    "coefs = [1] * n_trees\n",
    "\n",
    "trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZ2BU15lbsin",
    "outputId": "3223b2d8-6cba-451f-9af7-dfc89cc2fe22"
   },
   "outputs": [],
   "source": [
    "evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wuls3BYWbsip",
    "outputId": "3055e9f7-0063-40af-a6a7-849d06f24a7d"
   },
   "outputs": [],
   "source": [
    "get_error_plot(n_trees, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2gubWC0bsis"
   },
   "source": [
    "Теперь попробуем уменьшить шаг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqfMt3KMbsis"
   },
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "\n",
    "trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rkc97-ibsiu",
    "outputId": "c4a5a6e3-81d6-460f-ccaa-1ada3bcdb959"
   },
   "outputs": [],
   "source": [
    "evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0Ij4Fbrbsiw",
    "outputId": "9083ac69-d276-4faf-b01c-62403302c29c"
   },
   "outputs": [],
   "source": [
    "get_error_plot(n_trees, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOM9bowjbsiy"
   },
   "source": [
    "Видим, что качество обучения улучшается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCU1ZgSxbsi0"
   },
   "source": [
    "Уменьшим шаг до 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvDpXS-Ybsi1"
   },
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "\n",
    "trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7l4ckNubsi4",
    "outputId": "9ec51d03-e81f-4538-d27c-6af1e211c590"
   },
   "outputs": [],
   "source": [
    "evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0dPsV_9bsi5",
    "outputId": "ea0709a1-f420-4cbd-c7fe-75c7b5f64c79"
   },
   "outputs": [],
   "source": [
    "get_error_plot(n_trees, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTHcPMYhbsi7"
   },
   "source": [
    "При таком размере шага алгоритм сходится, но ему для достижения удовлетворительных показателей требуется большее количество итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoXzz4Fhbsi8"
   },
   "source": [
    "Вернемся к шагу 0.1 и попробуем увеличить глубину деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvLCSptsbsi8"
   },
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "max_depth = 5\n",
    "\n",
    "trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H493-X9Cbsi-",
    "outputId": "e77fb1e7-8723-49cb-de74-55806e798d02"
   },
   "outputs": [],
   "source": [
    "evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZGS4hIMbsjB",
    "outputId": "41d3b3a2-3d0a-4b82-d850-f9216261481a"
   },
   "outputs": [],
   "source": [
    "get_error_plot(n_trees, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjcLXlvZbsjI"
   },
   "source": [
    "В целом, тут мы показали, что варьируя параметры обучения градиентного бустинга можно добиваться различного уровня точности модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gn6K3xoIbsjJ"
   },
   "source": [
    "Существуют различные реализации градиентного бустинга, и одна из самых популярных и широко используемых - XGBoost (в Python содержится в библиотеке с аналогичным названием). С этой реализацией можно ознакомиться в дополнительных материалах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2tMUhhXbsjK"
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymxQoIaNbsjK"
   },
   "source": [
    "Алгоритм AdaBoost (Adaptive boosting) также в настоящее время является одним из подвидов градиентного бустинга, однако фактически он является своего рода \"предшественником\" алгоритма градиентного бустинга, после разработки которого став его частным случаем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-P-YoMabsjL"
   },
   "source": [
    "Для задачи бинарной классификации он заключается в использовании слабых классификаторов (например, деревьев глубиной 1 - так называемых \"пней\") в цикле, с придаванием объектам весов. После каждого шага итерации, когда разделяющая плоскость классификатора делит пространство объектов на две части, веса объектов перераспределяются, и веса неправильно классифицированных объектов увеличиваются, чтобы на следующей итерации классификатор акцентировался на этих объектах. Классификатору также присваивается вес в зависимости от его точности. Затем полученные деревья с весами объединяются в один сильный классификатор. В этом и заключается адаптивность алгоритма. Алгоритм AdaBoost также называют алгоритмом усиления классификаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0X9EilDubsjL"
   },
   "source": [
    "Таким образом, его алгоритм заключается в следующем:\n",
    "\n",
    "__1.__ Инициализация начальных весов объектов из выборки длиной $l$:\n",
    "\n",
    "$$D_{1}(i) = \\frac{1}{l}$$\n",
    "\n",
    "__2.__ Для каждого из $N$ деревьев в ансамбле:\n",
    "\n",
    "- находим классификатор $b_{n}$, который минимизирует взвешенную ошибку классификации\n",
    "\n",
    "$$b_{n} = \\underset{b}{\\text{argmin}}\\;\\varepsilon_{j},$$\n",
    "\n",
    "    где \n",
    "\n",
    "$$\\varepsilon_{j} = \\sum_{i=1}^{l}D_{n}(i)[y_{i}\\neq b_{j}(x)]$$\n",
    "\n",
    "- критерием остановки является значение $\\varepsilon_{j} \\geq 0.5$. При таком значении ошибки нужно выбрать другой классификатор и продолжить.\n",
    "\n",
    "- выбираем вес для дерева $\\alpha_{n}$ по формуле\n",
    "\n",
    "$$\\alpha_{n} = \\frac{1}{2}\\text{ln}\\frac{1 - \\varepsilon_{n}}{\\varepsilon_{n}}$$\n",
    "\n",
    "- обновляем веса при объектах:\n",
    "\n",
    "$$D_{n+1}(i) = \\frac{D_{n}(i)e^{-\\alpha_{n}y_{i}b_{n}(x_{i})}}{Z_{n}},$$\n",
    "\n",
    "выражение $y_{i}b_{n}(x_{i})$ в случае $Y = \\{-1, 1\\}$ будет равняться 1 для правильно классифицированных объектов и -1 для неправильно классифицированных, то есть по сути правильность классификации будет означать, будет $e^{\\alpha_{n}}$ стоять в числителе или в знаменателе формулы. В случае $Y = \\{0,1\\}$ вес будет уменьшаться у неправильно классифицированных объектов, а у неправильно классифицированных - оставаться неизменным (до нормализации). $Z_{n}$ здесь - нормализующий параметр, выбираемый так, чтобы $D_{n+1}$ по своей сути являлся распределением вероятностей, то есть\n",
    "\n",
    "$$\\sum_{i=1}^{l}D_{n+1} = 1.$$\n",
    "\n",
    "__3.__ Строим получившуюся модель\n",
    "\n",
    "$$a(x) = \\text{sign}\\left( \\sum_{n=1}^{N}\\alpha_{n}b_{n}(x) \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG5JA1ALbsjM"
   },
   "source": [
    "Напишем его простую реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMMYdEpSbsjM"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cQW-bglbsjP"
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbArvbGPbsjS"
   },
   "source": [
    "Разделим выборку на обучающую и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3Q2bSlibsjV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-YeAmXObsjZ"
   },
   "source": [
    "Реализуем функцию подсчета ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxVwBxTtbsja"
   },
   "outputs": [],
   "source": [
    "def get_error(pred, y):\n",
    "    return sum(pred != y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVRjV3Rubsjb"
   },
   "source": [
    "И сам алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiKx_qKObsjb"
   },
   "outputs": [],
   "source": [
    "def adaboost(X, y, N):\n",
    "\n",
    "    # Размер выборки\n",
    "    n_objects = len(X)\n",
    "\n",
    "    # Запишем количество классов в переменную\n",
    "    n_classes = len(np.unique((y)))\n",
    "\n",
    "    # Начальные веса деревьев\n",
    "    w = np.ones(n_objects) / n_objects\n",
    "\n",
    "    # Деревья с весами будем записывать в список\n",
    "    models = []\n",
    "\n",
    "    for n in range(N):\n",
    "        # Зададим дерево и обучим его\n",
    "        clf = DecisionTreeClassifier(max_depth=1)\n",
    "        clf.fit(X, y, sample_weight=w)\n",
    "\n",
    "        predictions = clf.predict(X)\n",
    "        e = get_error(predictions, y)\n",
    "        # отбросим дерево, если его ошибка больше 0.5\n",
    "        # Запишем условие в общем виде (применимо к небинарным классификаторам)\n",
    "        if e >= 1 - 1/n_classes: \n",
    "            break\n",
    "\n",
    "        # Вычислим вес для дерева\n",
    "        alpha = 0.5 * np.log((1 - e) / e)\n",
    "\n",
    "        # Найдем индексы правильно классифицированных элементов\n",
    "        match = predictions == y\n",
    "\n",
    "        # Увеличим веса для неправильно классифицированных элементов\n",
    "        w[~match] *= np.exp(alpha)\n",
    "\n",
    "        # Нормализуем веса\n",
    "        w /= w.sum()\n",
    "\n",
    "        # Добавим дерево с весом в список\n",
    "        models.append((alpha, clf))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCRBLGmYbsjd"
   },
   "source": [
    "Обучим алгоритм из 50 деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2QwLTjkbsje"
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "\n",
    "models = adaboost(X_train, y_train, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf6R7t9Wbsjg"
   },
   "source": [
    "Теперь осуществим предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cRftLvhbsjg",
    "outputId": "c89e1b01-378e-4247-cc75-1f2ea03412cc"
   },
   "outputs": [],
   "source": [
    "def predict(X, models):\n",
    "    \n",
    "    n_classes = 2\n",
    "    n_objects = len(X)\n",
    "    \n",
    "    # вначале обозначим предсказание нулевым массивом\n",
    "    y_pred = np.zeros((n_objects, n_classes))\n",
    "    \n",
    "    for alpha, clf in models:\n",
    "        prediction = clf.predict(X)\n",
    "        # Для каждого предсказания будем прибавлять alpha к\n",
    "        # элементу с индексом предсказанного класса\n",
    "        y_pred[range(n_objects), prediction] += alpha\n",
    "    \n",
    "    # выберем индексы с максимальными суммарными весами -\n",
    "    # получим предсказанные алгоритмом классы\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "print(f'Точность алгоритма на обучающей выборке: {(1 - get_error(predict(X_train, models), y_train)) * 100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIOQigknbsjj",
    "outputId": "eeb0e739-498b-4f52-bf6c-dc6bd9e71c08"
   },
   "outputs": [],
   "source": [
    "print(f'Точность алгоритма на тестовой выборке: {(1 - get_error(predict(X_test, models), y_test)) * 100:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64NptCn3bsjm"
   },
   "source": [
    "Построим графики зависимости ошибки от количества базовых алгоритмов в ансамбле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2diAmLwbsjn"
   },
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for n in range(1, 31):\n",
    "    mods = adaboost(X_train, y_train, n)\n",
    "    train_errors.append(get_error(predict(X_train, mods), y_train))\n",
    "    test_errors.append(get_error(predict(X_test, mods), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufw5bo9Lbsjp",
    "outputId": "9fdac949-034b-4855-dc60-992ec96a072c"
   },
   "outputs": [],
   "source": [
    "x = list(range(1, 31))\n",
    "\n",
    "plt.xlim(0, 30)\n",
    "plt.plot(x, train_errors, label='train errors')\n",
    "plt.plot(x, test_errors, label='test errors')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTqEbRmPbsjq"
   },
   "source": [
    "Достоинствами алгоритма AdaBoost можно назвать простоту реализации, хорошую обобщающую способность и небольшую вычислительную сложность. В то же время, есть и недостатки - в первую очередь, склонность к переобучению при наличии в данных шума и выбросов: для наиболее трудноклассифицируемых объектов алгоритм будет определять очень большие веса и в итоге переобучаться на них. В то же время это является и плюсом: таким образом можно идентифицировать выбросы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKn5SRC2bsjr"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6i9D-7Xbsjs"
   },
   "source": [
    "1. [Интерактивная демонстрация градиентного бустинга](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "2. [sklearn.datasets](https://scikit-learn.org/stable/datasets/index.html)\n",
    "3. [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "4. [$L_{1}$ loss и $L_{2}$ loss](https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions)\n",
    "5. [XGBoost](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture10-ensembles.pdf)\n",
    "6. [AdaBoost](https://ru.wikipedia.org/wiki/AdaBoost)\n",
    "7. [XGBoost: A Scalable Tree Boosting System - оригинальная статья](http://scholar.google.ru/scholar_url?url=https://dl.acm.org/ft_gateway.cfm%3Fftid%3D1775849%26id%3D2939785&hl=en&sa=X&scisig=AAGBfm3b8fqJWtjjjejQ5fQwrtg9eQQK-w&nossl=1&oi=scholarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* На больших и сложных данных градиентный бустинг - один из лучших алгоритмов\n",
    "* Много настраиваемых параметров \n",
    "* Есть очень быстрые реализации \n",
    "* Обычно строят на деревьях решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Д/з"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для реализованной модели градиентного бустинга построить графики зависимости ошибки от количества деревьев в ансамбле и от максимальной глубины деревьев. Сделать выводы о зависимости ошибки от этих параметров. \n",
    "2. Модифицировать реализованный алгоритм, чтобы получился стохастический градиентный бустинг. Размер подвыборки принять равным 0.5. Сравнить на одном графике кривые изменения ошибки на тестовой выборке в зависимости от числа итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Деревья, леса, бустинги, я начинаю путаться, как нам это всё учить и какую цель мы преследуем?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L6_Learning_Curves.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Давайте еще раз суть бустинга для регресии/классификации и как настройка параметров влияет на качество?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L6_GM_Reg.png\" style=\"width: 600px;\">\n",
    "\n",
    "<img src=\"images/L6_GM_Class.png\" style=\"width: 600px;\">\n",
    "\n",
    "<img src=\"images/L6_GM_Plots.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. XGBoost / LightGBM / CatBoost - три крайности одной и той же сущности?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __XGBoost__ (2014) - библиотека для обучения моделей градиентного бустинга XGBoost ~ Extreme Gradient Boosting\n",
    "    * Особое внимание уделено регуляризации моделей\n",
    "    \n",
    "    \n",
    "* __LightGBM__ (2017) - библиотека для обучения градиентного бустинга от Microsoft\n",
    "    * Для оценки важности примера используется метод GOSS (Gradient-based One-Side Sampling) - чем больше градиент функции потерь на объекте, тем объект “сложнее”\n",
    "    * Деревья строятся “в глубину” (Leaf-wise tree growth)\n",
    "\n",
    "\n",
    "* __CatBoost__ (2017) - библиотека для обучения градиентного бустинга от Яндекса\n",
    "    * Особое внимание уделяется работе с категориальными признаками (разные способы кодирования - с использованием целевой переменной и без)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L6_boostings.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Есть ли бустинги, основанные не на весах объектов?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaBoost - это __Weight-based boosting__ - бустинг, основанный на весах объектов\n",
    "* Есть еще __Residual-based boosting__ - бустинг, основанный на остатках\n",
    "    * Шаг 1: строим модель b1(x)\n",
    "        * Делаем предсказания с помощью модели b1(x)\n",
    "        * Считаем (для каждого объекта) невязку (residal) модели b1(x) как y - y_pred_1 (не абсолютная ошибка!)\n",
    "        * Делаем полученную невязку новой целевой переменной (new y) для обучения следующей модели\n",
    "    * Шаг 2: строим модель b2(x) для предсказания невязки первой модели\n",
    "        * Делаем предсказания с помощью модели b2(x)\n",
    "        * Считаем (для каждого объекта) невязку (residal) модели b2(x) как y_1 - y_pred_2 \n",
    "        * Делаем полученную невязку новой целевой переменной (new y) для обучения следующей модели\n",
    "    * То же для b3(x), b4(x), ..., bT(x)\n",
    "    * В построенной композиции модель bi(x) аппроксимирует невязку между целевой переменной и ответами всех предыдущих i-1 моделей!\n",
    "    * В итоге получаем итоговый ответ, как сумму моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Что будет с выбросами в обучающей выборке при применении AdaBoost и как с этим бороться?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L6_Q5.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Можно ли делать оценку out-of-bag (OOB) при стохастическом градиентном бустинге?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, можно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Как реализована регуляризация деревьев в XGBoost?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/L6_Q7.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Как реализовано распараллеливание в XGBoost (параметр n_jobs)?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Паралеллится построение самого дерева на каждой итерации бустинга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. Можно ли как-то \"усилить\" модель бустинга?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending\n",
    "<img src=\"images/L6_blending_info.png\" style=\"width: 800px;\">\n",
    "<img src=\"images/L6_blending.png\" style=\"width: 700px;\">\n",
    "\n",
    "### Stacking\n",
    "<img src=\"images/L6_stacking_info.png\" style=\"width: 800px;\">\n",
    "<img src=\"images/L6_stacking.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Rg5ilDbTbshs",
    "Z2tMUhhXbsjK",
    "gKn5SRC2bsjr",
    "qBVboqCmbsjs"
   ],
   "name": "Lesson_6(edited).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
